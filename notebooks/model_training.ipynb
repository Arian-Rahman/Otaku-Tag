{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Synopsis</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Boys Love</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Ecchi</th>\n",
       "      <th>Erotica</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Girls Love</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Slice of Life</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Supernatural</th>\n",
       "      <th>Suspense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shingeki no Kyojin</td>\n",
       "      <td>Centuries ago, mankind was slaughtered to near...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>After a horrific alchemy experiment goes wrong...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One Punch Man</td>\n",
       "      <td>The seemingly unimpressive Saitama has a rathe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kimetsu no Yaiba</td>\n",
       "      <td>Ever since the death of his father, the burden...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sword Art Online</td>\n",
       "      <td>Ever since the release of the innovative Nerve...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Title  \\\n",
       "0                Shingeki no Kyojin   \n",
       "1  Fullmetal Alchemist: Brotherhood   \n",
       "2                     One Punch Man   \n",
       "3                  Kimetsu no Yaiba   \n",
       "4                  Sword Art Online   \n",
       "\n",
       "                                            Synopsis  Action  Adventure  \\\n",
       "0  Centuries ago, mankind was slaughtered to near...     1.0        0.0   \n",
       "1  After a horrific alchemy experiment goes wrong...     1.0        1.0   \n",
       "2  The seemingly unimpressive Saitama has a rathe...     1.0        0.0   \n",
       "3  Ever since the death of his father, the burden...     1.0        0.0   \n",
       "4  Ever since the release of the innovative Nerve...     1.0        1.0   \n",
       "\n",
       "   Boys Love  Comedy  Drama  Ecchi  Erotica  Fantasy  Girls Love  Horror  \\\n",
       "0        0.0     0.0    1.0    0.0      0.0      0.0         0.0     0.0   \n",
       "1        0.0     0.0    1.0    0.0      0.0      1.0         0.0     0.0   \n",
       "2        0.0     1.0    0.0    0.0      0.0      0.0         0.0     0.0   \n",
       "3        0.0     0.0    0.0    0.0      0.0      0.0         0.0     0.0   \n",
       "4        0.0     0.0    0.0    0.0      0.0      1.0         0.0     0.0   \n",
       "\n",
       "   Mystery  Romance  Sci-Fi  Slice of Life  Sports  Supernatural  Suspense  \n",
       "0      0.0      0.0     0.0            0.0     0.0           0.0       1.0  \n",
       "1      0.0      0.0     0.0            0.0     0.0           0.0       0.0  \n",
       "2      0.0      0.0     0.0            0.0     0.0           0.0       0.0  \n",
       "3      0.0      0.0     0.0            0.0     0.0           1.0       0.0  \n",
       "4      0.0      1.0     0.0            0.0     0.0           0.0       0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_path = os.path.join(\"..\",\"data\",\"cleaned\",\"clean_combined_data.csv\")\n",
    "\n",
    "df = pd.read_csv(raw_data_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Backup \n",
    "df_backup_1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Backup \n",
    "df = df_backup_1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing & Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer-Based Models (PyTorch + Hugging Face) + ULMFiT (FastAI)\n",
    "These models use **subword tokenization** (e.g., WordPiece, Byte-Pair Encoding) and are powerful for NLP tasks.\n",
    "\n",
    "| **Model**              | **Tokenizer** | **Max Tokens** | **Strengths** | **Weaknesses** |\n",
    "|------------------------|--------------|---------------|--------------|---------------|\n",
    "| **BERT (bert-base-uncased)** | WordPiece | 512 | General-purpose, bidirectional context, good for sentence-level tasks. | Limited to 512 tokens, slow training. |\n",
    "| **DistilBERT (distilbert-base-uncased)** | WordPiece | 512 | 40% fewer parameters than BERT, faster inference. | Slightly lower accuracy than BERT. |\n",
    "| **RoBERTa (roberta-base)** | Byte-Pair Encoding (BPE) | 512 | Trained on more data than BERT, better generalization. | Needs more GPU memory than DistilBERT. |\n",
    "| **ALBERT (albert-base-v2)** | SentencePiece | 512 | Compressed BERT with shared parameters, faster training. | Some accuracy trade-offs. |\n",
    "| **XLNet (xlnet-base-cased)** | SentencePiece | 512 | Uses permutation-based learning, good for long sequences. | More complex training, slow inference. |\n",
    "| **Longformer (allenai/longformer-base-4096)** | Byte-Pair Encoding (BPE) | 4,096 | Handles long documents, sliding window attention. | Expensive to train, high VRAM usage. |\n",
    "| **GPT-2 (gpt2)** | Byte-Pair Encoding (BPE) | 1,024 | Strong generative capabilities, good for text completion. | Unidirectional, may not work well for classification. |\n",
    "| **T5 (t5-small)** | SentencePiece | 512 | Can handle multiple NLP tasks (classification, summarization, translation). | Needs task-specific tuning, larger models are VRAM-intensive. |\n",
    "| **BART (facebook/bart-base)** | Byte-Pair Encoding (BPE) | 1,024 | Seq2Seq-based transformer, good for summarization & text classification. | Requires more computational resources. |\n",
    "| **ULMFiT (AWD-LSTM - fastai)** | FastAI/Spacy Tokenizer | No fixed limit | Good for small datasets, efficient training. | Not as powerful as transformers, may struggle with unseen words. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Token Length: 1001\n",
      "Average Token Length: 103.47926452242773\n"
     ]
    }
   ],
   "source": [
    "# Find average and max token length\n",
    "\n",
    "df_test = df_backup_1.copy()\n",
    "\n",
    "# Choose tokenizer \n",
    "MODEL_NAME = \"distilbert-base-uncased\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "token_lengths = df_test[\"Synopsis\"].dropna().apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "\n",
    "# Compute max and average token lengths\n",
    "max_length = token_lengths.max()\n",
    "avg_length = token_lengths.mean()\n",
    "\n",
    "print(f\"Max Token Length: {max_length}\")\n",
    "print(f\"Average Token Length: {avg_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>92185.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>103.479265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>77.633858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>94.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1001.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Token Length\n",
       "count  92185.000000\n",
       "mean     103.479265\n",
       "std       77.633858\n",
       "min        1.000000\n",
       "25%       39.000000\n",
       "50%       94.000000\n",
       "75%      150.000000\n",
       "max     1001.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can convert token_lengths to a DataFrame and use the hist method to plot a histogram of the token lengths.\n",
    "token_df = pd.DataFrame(token_lengths)\n",
    "token_df.columns = [\"Token Length\"]\n",
    "token_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** \n",
    "\n",
    "Since 3rd STD value is 150 , safe to say most summaries are short. \n",
    "\n",
    "Since there seem to be some outliars , we need to remove them to \n",
    "\n",
    "make the df compatible for 512 token based models . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot histogram of token lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(token_df[\"Token Length\"], bins=\"auto\", kde=True, discrete=True)\n",
    "plt.title(\"Distribution of Token Lengths\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "It seems like we have a very large peak near 10 . \n",
    "\n",
    "It looks unnatural for the kind of work we are doing. \n",
    "\n",
    "I need to zoom in a little bit . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot histogram of token lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(token_df[\"Token Length\"], bins=\"auto\", kde=True, discrete=True)\n",
    "plt.title(\"Distribution of Token Lengths\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Set x-axis limits to zoom in\n",
    "plt.xlim(0, 100)  \n",
    "\n",
    "plt.xticks(np.arange(0, 101, 5))  # Adjust the step size as needed\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "Looks like the peak is at token size 7 . \n",
    "\n",
    "so we need to see what that sentance actually is that is repeating so many times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenizer to get token lengths and filter rows with exactly 7 tokens\n",
    "synopsis_with_7_tokens = df_test[\"Synopsis\"].apply(lambda x: len(tokenizer.tokenize(str(x))) == 7)\n",
    "\n",
    "# Get the original synopses that have exactly 7 tokens\n",
    "original_synopsis_with_7_tokens = df_test[synopsis_with_7_tokens].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of data with 7 tokens: 8.98%\n"
     ]
    }
   ],
   "source": [
    "percent_of_sus_data=(original_synopsis_with_7_tokens.shape[0]/(df_test.shape[0]+ original_synopsis_with_7_tokens.shape[0]))*100\n",
    "\n",
    "print(f\"Percentage of data with 7 tokens: {percent_of_sus_data:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Synopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>One Punch Man 3</td>\n",
       "      <td>Third season of One Punch Man.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>Spy x Family Season 3</td>\n",
       "      <td>Third season of Spy x Family.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>Dandadan 2nd Season</td>\n",
       "      <td>Second season of Dandadan.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Shinmai Maou no Testament Departures</td>\n",
       "      <td>(No synopsis yet.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>Overlord Movie 1: Fushisha no Ou</td>\n",
       "      <td>First Overlord recap film.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>Strike the Blood IV</td>\n",
       "      <td>Fourth season of Strike the Blood.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>Overlord Movie 2: Shikkoku no Eiyuu</td>\n",
       "      <td>Second Overlord recap film.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>Break Blade Movie 6: Doukoku no Toride</td>\n",
       "      <td>Sixth and final Break Blade movie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>Cencoroll Connect</td>\n",
       "      <td>The sequel to Cencoroll.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>Strike the Blood Final</td>\n",
       "      <td>Fifth season of Strike the Blood.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1672</th>\n",
       "      <td>Drifters: Special Edition</td>\n",
       "      <td>Bundled with the fifth manga volume.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711</th>\n",
       "      <td>Sword Art Online (Original Movie)</td>\n",
       "      <td>(No synopsis yet.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>New Panty &amp; Stocking with Garterbelt</td>\n",
       "      <td>(No synopsis yet.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>Boruto: Naruto Next Generations Part 2</td>\n",
       "      <td>(No synopsis yet.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>Dorohedoro (Zoku-hen)</td>\n",
       "      <td>Sequel to Dorohedoro.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>Ishura 2nd Season</td>\n",
       "      <td>Second season of Ishura.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>Dies Irae: Reimei</td>\n",
       "      <td>Episode 0 of Dies Irae.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>Golden Kamuy: Saishuushou</td>\n",
       "      <td>Final chapter of Golden Kamuy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2326</th>\n",
       "      <td>Jormungand: Perfect Order Recap</td>\n",
       "      <td>A recap of season 1.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2365</th>\n",
       "      <td>Beatless Intermission</td>\n",
       "      <td>Recap episodes of Beatless.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2391</th>\n",
       "      <td>Nurarihyon no Mago: Sennen Makyou Recaps</td>\n",
       "      <td>(No synopsis yet.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>Katsugeki/Touken Ranbu Movie</td>\n",
       "      <td>(No synopsis yet.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>Arknights: Rise from Ember</td>\n",
       "      <td>New season of Arknights.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>Yao Shen Ji 2nd Season</td>\n",
       "      <td>Second season of Yao Shen Ji.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>Shuo Feng: Po Zhen Zi</td>\n",
       "      <td>(No synopsis yet.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2587</th>\n",
       "      <td>Yaoguai Mingdan 2nd Season</td>\n",
       "      <td>(No synopsis yet.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2617</th>\n",
       "      <td>Yao Shen Ji 3rd Season</td>\n",
       "      <td>Third season of Yao Shen Ji.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>Kidou Senshi Gundam: Senkou no Hathaway 2</td>\n",
       "      <td>(No synopsis yet.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2711</th>\n",
       "      <td>Soukyuu no Fafner: Dead Aggressor - The Beyond</td>\n",
       "      <td>(No synopsis yet.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2789</th>\n",
       "      <td>Yao Shen Ji 4th Season</td>\n",
       "      <td>Fourth season of Yao Shen Ji.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>Kidou Keisatsu Patlabor: Reboot</td>\n",
       "      <td>(No synopsis yet.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2867</th>\n",
       "      <td>Kidou Senshi Gundam: Senkou no Hathaway 3</td>\n",
       "      <td>(No synopsis yet.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2868</th>\n",
       "      <td>Hakuouki OVA (2021)</td>\n",
       "      <td>New Hakuoki OVA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3110</th>\n",
       "      <td>Yao Shen Ji 5th Season</td>\n",
       "      <td>Fifth season of Yao Shen Ji.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3137</th>\n",
       "      <td>Precure Dream Stars! Movie</td>\n",
       "      <td>(No synopsis yet.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>Precure All Stars Movie: Haru no Carnival</td>\n",
       "      <td>New Precure All Stars movie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3215</th>\n",
       "      <td>Precure Super Stars! Movie</td>\n",
       "      <td>(No synopsis yet.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3217</th>\n",
       "      <td>TheUltraman</td>\n",
       "      <td>The first animated Ultraman series.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3286</th>\n",
       "      <td>Peace Maker Kurogane Movie 2: Yuumei</td>\n",
       "      <td>(No synopsis yet.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3294</th>\n",
       "      <td>Monster Strike the Movie: Lucifer - Zetsubou n...</td>\n",
       "      <td>(No synopsis yet.)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  \\\n",
       "444                                     One Punch Man 3   \n",
       "916                               Spy x Family Season 3   \n",
       "934                                 Dandadan 2nd Season   \n",
       "995                Shinmai Maou no Testament Departures   \n",
       "1033                   Overlord Movie 1: Fushisha no Ou   \n",
       "1038                                Strike the Blood IV   \n",
       "1060                Overlord Movie 2: Shikkoku no Eiyuu   \n",
       "1354             Break Blade Movie 6: Doukoku no Toride   \n",
       "1559                                  Cencoroll Connect   \n",
       "1577                             Strike the Blood Final   \n",
       "1672                          Drifters: Special Edition   \n",
       "1711                  Sword Art Online (Original Movie)   \n",
       "1713               New Panty & Stocking with Garterbelt   \n",
       "1801             Boruto: Naruto Next Generations Part 2   \n",
       "1851                              Dorohedoro (Zoku-hen)   \n",
       "1867                                  Ishura 2nd Season   \n",
       "1889                                  Dies Irae: Reimei   \n",
       "1915                          Golden Kamuy: Saishuushou   \n",
       "2326                    Jormungand: Perfect Order Recap   \n",
       "2365                              Beatless Intermission   \n",
       "2391           Nurarihyon no Mago: Sennen Makyou Recaps   \n",
       "2478                       Katsugeki/Touken Ranbu Movie   \n",
       "2502                         Arknights: Rise from Ember   \n",
       "2564                             Yao Shen Ji 2nd Season   \n",
       "2573                              Shuo Feng: Po Zhen Zi   \n",
       "2587                         Yaoguai Mingdan 2nd Season   \n",
       "2617                             Yao Shen Ji 3rd Season   \n",
       "2650          Kidou Senshi Gundam: Senkou no Hathaway 2   \n",
       "2711     Soukyuu no Fafner: Dead Aggressor - The Beyond   \n",
       "2789                             Yao Shen Ji 4th Season   \n",
       "2830                    Kidou Keisatsu Patlabor: Reboot   \n",
       "2867          Kidou Senshi Gundam: Senkou no Hathaway 3   \n",
       "2868                                Hakuouki OVA (2021)   \n",
       "3110                             Yao Shen Ji 5th Season   \n",
       "3137                         Precure Dream Stars! Movie   \n",
       "3166         Precure All Stars Movie: Haru no Carnival   \n",
       "3215                         Precure Super Stars! Movie   \n",
       "3217                                       TheUltraman   \n",
       "3286               Peace Maker Kurogane Movie 2: Yuumei   \n",
       "3294  Monster Strike the Movie: Lucifer - Zetsubou n...   \n",
       "\n",
       "                                  Synopsis  \n",
       "444         Third season of One Punch Man.  \n",
       "916          Third season of Spy x Family.  \n",
       "934             Second season of Dandadan.  \n",
       "995                     (No synopsis yet.)  \n",
       "1033            First Overlord recap film.  \n",
       "1038    Fourth season of Strike the Blood.  \n",
       "1060           Second Overlord recap film.  \n",
       "1354    Sixth and final Break Blade movie.  \n",
       "1559              The sequel to Cencoroll.  \n",
       "1577     Fifth season of Strike the Blood.  \n",
       "1672  Bundled with the fifth manga volume.  \n",
       "1711                    (No synopsis yet.)  \n",
       "1713                    (No synopsis yet.)  \n",
       "1801                    (No synopsis yet.)  \n",
       "1851                 Sequel to Dorohedoro.  \n",
       "1867              Second season of Ishura.  \n",
       "1889               Episode 0 of Dies Irae.  \n",
       "1915        Final chapter of Golden Kamuy.  \n",
       "2326                  A recap of season 1.  \n",
       "2365           Recap episodes of Beatless.  \n",
       "2391                    (No synopsis yet.)  \n",
       "2478                    (No synopsis yet.)  \n",
       "2502              New season of Arknights.  \n",
       "2564         Second season of Yao Shen Ji.  \n",
       "2573                    (No synopsis yet.)  \n",
       "2587                    (No synopsis yet.)  \n",
       "2617          Third season of Yao Shen Ji.  \n",
       "2650                    (No synopsis yet.)  \n",
       "2711                    (No synopsis yet.)  \n",
       "2789         Fourth season of Yao Shen Ji.  \n",
       "2830                    (No synopsis yet.)  \n",
       "2867                    (No synopsis yet.)  \n",
       "2868                      New Hakuoki OVA.  \n",
       "3110          Fifth season of Yao Shen Ji.  \n",
       "3137                    (No synopsis yet.)  \n",
       "3166          New Precure All Stars movie.  \n",
       "3215                    (No synopsis yet.)  \n",
       "3217   The first animated Ultraman series.  \n",
       "3286                    (No synopsis yet.)  \n",
       "3294                    (No synopsis yet.)  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_synopsis_with_7_tokens.iloc[:40, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "From the looks of it,the synopsis with data such as (No Synopsis yet.) and \"nth Season of X anime\" \n",
    "\n",
    "Are of no use for the classification , so we need to find a threshold where the token size indicates\n",
    "\n",
    "useful synopsis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopsis_with_32_tokens = df_test[\"Synopsis\"].apply(lambda x: len(tokenizer.tokenize(str(x))) == 32)\n",
    "original_synopsis_with_32_tokens = df_test[synopsis_with_32_tokens].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Synopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>Chuunibyou demo Koi ga Shitai! Depth of Field:...</td>\n",
       "      <td>Specials bundled with the first season's BD/DV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>Toaru Majutsu no Index-tan Movie: Endymion no ...</td>\n",
       "      <td>Bonus animation featuring Index-tan included w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935</th>\n",
       "      <td>All That Gundam</td>\n",
       "      <td>Animated video created just for a 1989 event t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3028</th>\n",
       "      <td>Kyoukaisenjou no Horizon Special</td>\n",
       "      <td>The Blu-ray Box set of Kyoukaisenjou no Horizo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3061</th>\n",
       "      <td>Cobra The Animation: Time Drive</td>\n",
       "      <td>Lady has disappeared so Cobra must dive into h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269</th>\n",
       "      <td>Ku Pao Ying Xiong</td>\n",
       "      <td>The animation tells the story of Xiao Shuai wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>Bari Bari Densetsu (1987)</td>\n",
       "      <td>The film is about 4 high-schoolers who are bik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3431</th>\n",
       "      <td>Super Robot Taisen OG: Divine Wars - Sorezore ...</td>\n",
       "      <td>This is considered episode 26 of the series wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3650</th>\n",
       "      <td>Aru Zombie Shoujo no Sainan PV</td>\n",
       "      <td>Promotional video for an anime adaptation of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3775</th>\n",
       "      <td>Kung Fu Liaoli Niang</td>\n",
       "      <td>A young, starving traveler finds himself in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>Star Ocean: Anamnesis</td>\n",
       "      <td>Fully animated trailers for Square Enix's Star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4407</th>\n",
       "      <td>Master of Torque 3</td>\n",
       "      <td>At the end of the Four Seasons: Fuuka and Nana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4432</th>\n",
       "      <td>Boogiepop and Others: Promotional Trailer</td>\n",
       "      <td>Promotional video posted on Kadokawa's officia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4576</th>\n",
       "      <td>Tatakae! Osper</td>\n",
       "      <td>On a distant world, people with paranormal pow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4731</th>\n",
       "      <td>Kinnikuman no Koutsuu Anzen</td>\n",
       "      <td>A traffic safety film starring the Kinnikuman ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>Fly With Me</td>\n",
       "      <td>Music video for the song Fly with me by millen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6721</th>\n",
       "      <td>Doraemon Movie 40: Nobita no Shin Kyouryuu</td>\n",
       "      <td>Nobita and the gang travel 66 million years in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6752</th>\n",
       "      <td>Cobra The Animation: Time Drive</td>\n",
       "      <td>Lady has disappeared so Cobra must dive into h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6901</th>\n",
       "      <td>Ku Pao Ying Xiong</td>\n",
       "      <td>The animation tells the story of Xiao Shuai wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7017</th>\n",
       "      <td>Muumin (1972)</td>\n",
       "      <td>A remake of the 1969-70 Moomin series. This ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7178</th>\n",
       "      <td>Defend Love</td>\n",
       "      <td>The mysterious alter ego of Amuro gets sent to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7300</th>\n",
       "      <td>Yama Nezumi Rocky Chuck</td>\n",
       "      <td>Various stories about a group of animals that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7420</th>\n",
       "      <td>Don Chuck Monogatari</td>\n",
       "      <td>Young beaver Chuck's adventure with his little...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7520</th>\n",
       "      <td>Meitantei Conan: Keisatsu Gakkou-hen Wild Poli...</td>\n",
       "      <td>Final epsiode of Keisatsu Gakkou-hen Wild Poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7721</th>\n",
       "      <td>Stitch!: Itazura Alien no Daibouken - Uchuu Ic...</td>\n",
       "      <td>An exclusive episode showing the guest charact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7752</th>\n",
       "      <td>HappyLucky Bikkuriman</td>\n",
       "      <td>Continuation of the Bikkuriman Series.\\n\\nGenk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7866</th>\n",
       "      <td>Kirin the Noop</td>\n",
       "      <td>A shadow painting anime about a timid giraffe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7971</th>\n",
       "      <td>Nihon Meisaku Douwa Series: Akai Tori no Kokoro</td>\n",
       "      <td>A collection of children's stories originally ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8232</th>\n",
       "      <td>Straw Byururu</td>\n",
       "      <td>Byururu came from the straw world. At night th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8266</th>\n",
       "      <td>Mo Jing Lieren</td>\n",
       "      <td>The series takes place on a fantastical world ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8368</th>\n",
       "      <td>Mo Jing Lieren 3rd Season</td>\n",
       "      <td>The series takes place on a fantastical world ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>Maboroshi Mabo-chan</td>\n",
       "      <td>Mabo-chan goes on a world-wide journey to find...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8379</th>\n",
       "      <td>Jarujio Animal</td>\n",
       "      <td>The real female idol group PASSPO are the sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8414</th>\n",
       "      <td>Mo Jing Lieren 2nd Season</td>\n",
       "      <td>The series takes place on a fantastical world ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9457</th>\n",
       "      <td>Fly With Me</td>\n",
       "      <td>Music video for the song Fly with me by millen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11386</th>\n",
       "      <td>Ai Tenchi Muyou!</td>\n",
       "      <td>The franchise is celebrating its 20th annivers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11398</th>\n",
       "      <td>Shinmai Maou no Testament Burst Specials</td>\n",
       "      <td>Specials included on the Blu-ray/DVD volumes. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11460</th>\n",
       "      <td>MM! Specials</td>\n",
       "      <td>The six Blu-ray Disc/DVD volumes for the MM! t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11840</th>\n",
       "      <td>Boku no Kokoro no Yabai Yatsu Movie</td>\n",
       "      <td>A re-edit of both anime seasons featuring newl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11976</th>\n",
       "      <td>Bokura wa Minna Kawai-sou Specials</td>\n",
       "      <td>Based on the 4-koma manga stories of Bokura wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "1052   Chuunibyou demo Koi ga Shitai! Depth of Field:...   \n",
       "1892   Toaru Majutsu no Index-tan Movie: Endymion no ...   \n",
       "2935                                     All That Gundam   \n",
       "3028                    Kyoukaisenjou no Horizon Special   \n",
       "3061                     Cobra The Animation: Time Drive   \n",
       "3269                                   Ku Pao Ying Xiong   \n",
       "3289                           Bari Bari Densetsu (1987)   \n",
       "3431   Super Robot Taisen OG: Divine Wars - Sorezore ...   \n",
       "3650                      Aru Zombie Shoujo no Sainan PV   \n",
       "3775                                Kung Fu Liaoli Niang   \n",
       "4236                               Star Ocean: Anamnesis   \n",
       "4407                                  Master of Torque 3   \n",
       "4432           Boogiepop and Others: Promotional Trailer   \n",
       "4576                                      Tatakae! Osper   \n",
       "4731                         Kinnikuman no Koutsuu Anzen   \n",
       "4843                                         Fly With Me   \n",
       "6721          Doraemon Movie 40: Nobita no Shin Kyouryuu   \n",
       "6752                     Cobra The Animation: Time Drive   \n",
       "6901                                   Ku Pao Ying Xiong   \n",
       "7017                                       Muumin (1972)   \n",
       "7178                                         Defend Love   \n",
       "7300                             Yama Nezumi Rocky Chuck   \n",
       "7420                                Don Chuck Monogatari   \n",
       "7520   Meitantei Conan: Keisatsu Gakkou-hen Wild Poli...   \n",
       "7721   Stitch!: Itazura Alien no Daibouken - Uchuu Ic...   \n",
       "7752                              HappyLucky Bikkuriman   \n",
       "7866                                      Kirin the Noop   \n",
       "7971     Nihon Meisaku Douwa Series: Akai Tori no Kokoro   \n",
       "8232                                       Straw Byururu   \n",
       "8266                                      Mo Jing Lieren   \n",
       "8368                           Mo Jing Lieren 3rd Season   \n",
       "8373                                 Maboroshi Mabo-chan   \n",
       "8379                                      Jarujio Animal   \n",
       "8414                           Mo Jing Lieren 2nd Season   \n",
       "9457                                         Fly With Me   \n",
       "11386                                   Ai Tenchi Muyou!   \n",
       "11398           Shinmai Maou no Testament Burst Specials   \n",
       "11460                                       MM! Specials   \n",
       "11840                Boku no Kokoro no Yabai Yatsu Movie   \n",
       "11976                 Bokura wa Minna Kawai-sou Specials   \n",
       "\n",
       "                                                Synopsis  \n",
       "1052   Specials bundled with the first season's BD/DV...  \n",
       "1892   Bonus animation featuring Index-tan included w...  \n",
       "2935   Animated video created just for a 1989 event t...  \n",
       "3028   The Blu-ray Box set of Kyoukaisenjou no Horizo...  \n",
       "3061   Lady has disappeared so Cobra must dive into h...  \n",
       "3269   The animation tells the story of Xiao Shuai wh...  \n",
       "3289   The film is about 4 high-schoolers who are bik...  \n",
       "3431   This is considered episode 26 of the series wh...  \n",
       "3650   Promotional video for an anime adaptation of t...  \n",
       "3775   A young, starving traveler finds himself in th...  \n",
       "4236   Fully animated trailers for Square Enix's Star...  \n",
       "4407   At the end of the Four Seasons: Fuuka and Nana...  \n",
       "4432   Promotional video posted on Kadokawa's officia...  \n",
       "4576   On a distant world, people with paranormal pow...  \n",
       "4731   A traffic safety film starring the Kinnikuman ...  \n",
       "4843   Music video for the song Fly with me by millen...  \n",
       "6721   Nobita and the gang travel 66 million years in...  \n",
       "6752   Lady has disappeared so Cobra must dive into h...  \n",
       "6901   The animation tells the story of Xiao Shuai wh...  \n",
       "7017   A remake of the 1969-70 Moomin series. This ve...  \n",
       "7178   The mysterious alter ego of Amuro gets sent to...  \n",
       "7300   Various stories about a group of animals that ...  \n",
       "7420   Young beaver Chuck's adventure with his little...  \n",
       "7520   Final epsiode of Keisatsu Gakkou-hen Wild Poli...  \n",
       "7721   An exclusive episode showing the guest charact...  \n",
       "7752   Continuation of the Bikkuriman Series.\\n\\nGenk...  \n",
       "7866   A shadow painting anime about a timid giraffe ...  \n",
       "7971   A collection of children's stories originally ...  \n",
       "8232   Byururu came from the straw world. At night th...  \n",
       "8266   The series takes place on a fantastical world ...  \n",
       "8368   The series takes place on a fantastical world ...  \n",
       "8373   Mabo-chan goes on a world-wide journey to find...  \n",
       "8379   The real female idol group PASSPO are the sta...  \n",
       "8414   The series takes place on a fantastical world ...  \n",
       "9457   Music video for the song Fly with me by millen...  \n",
       "11386  The franchise is celebrating its 20th annivers...  \n",
       "11398  Specials included on the Blu-ray/DVD volumes. ...  \n",
       "11460  The six Blu-ray Disc/DVD volumes for the MM! t...  \n",
       "11840  A re-edit of both anime seasons featuring newl...  \n",
       "11976  Based on the 4-koma manga stories of Bokura wa...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_synopsis_with_32_tokens.iloc[:40, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data size: 93144\n",
      "Data size with less than 31 tokens: 21151\n",
      "Percentage of data with less than 31 tokens: 18.51%\n",
      "Total data size after filtering: 71993\n"
     ]
    }
   ],
   "source": [
    "synopsis_with_less_than_32_tokens = df_test[\"Synopsis\"].apply(lambda x: len(tokenizer.tokenize(str(x))) < 31)\n",
    "original_synopsis_with_less_than_32_tokens = df_test[synopsis_with_less_than_32_tokens].copy()\n",
    "\n",
    "percent_less_than_32_token=(original_synopsis_with_less_than_32_tokens.shape[0]/(df_test.shape[0]+ original_synopsis_with_less_than_32_tokens.shape[0]))*100\n",
    "\n",
    "print(f\"Original data size: {df_test.shape[0]}\")\n",
    "print(f\"Data size with less than 31 tokens: {original_synopsis_with_less_than_32_tokens.shape[0]}\")\n",
    "print(f\"Percentage of data with less than 31 tokens: {percent_less_than_32_token:.2f}%\")\n",
    "print(f\"Total data size after filtering: {df_test.shape[0] - original_synopsis_with_less_than_32_tokens.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "71,993 is still plenty of data for our task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "Token size of 32 and above seems to contain synopsis with useful information \n",
    "\n",
    "for classification . So we will only keep the data that is greater or equal \n",
    "\n",
    "to 32 tokens and less than or equal to 512 tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 93144\n",
      "Filtered dataset size: 71548\n"
     ]
    }
   ],
   "source": [
    "MAX_TOKENS = 512\n",
    "MIN_TOKENS = 32\n",
    "\n",
    "# Function to safely tokenize and handle non-strings\n",
    "def get_token_length(text):\n",
    "    try:\n",
    "        return len(tokenizer.tokenize(str(text)))\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing text: {text}, {e}\")\n",
    "        return 0 \n",
    "\n",
    "# Compute token lengths for all \"Synopsis\" entries, ensuring non-null values\n",
    "mask = df[\"Synopsis\"].notna() & (df[\"Synopsis\"].apply(get_token_length) <= MAX_TOKENS) & (df[\"Synopsis\"].apply(get_token_length) >= MIN_TOKENS)\n",
    "\n",
    "# Filter rows where the token length is within the min and max limits\n",
    "df_filtered_tokens = df[mask].copy()  # Avoids SettingWithCopyWarning\n",
    "\n",
    "# Print summary\n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "print(f\"Filtered dataset size: {len(df_filtered_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 71548 entries, 0 to 92196\n",
      "Data columns (total 19 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Title          71548 non-null  object \n",
      " 1   Synopsis       71548 non-null  object \n",
      " 2   Action         70904 non-null  float64\n",
      " 3   Adventure      70904 non-null  float64\n",
      " 4   Boys Love      70904 non-null  float64\n",
      " 5   Comedy         70904 non-null  float64\n",
      " 6   Drama          70904 non-null  float64\n",
      " 7   Ecchi          70904 non-null  float64\n",
      " 8   Erotica        70904 non-null  float64\n",
      " 9   Fantasy        70904 non-null  float64\n",
      " 10  Girls Love     70904 non-null  float64\n",
      " 11  Horror         70904 non-null  float64\n",
      " 12  Mystery        70904 non-null  float64\n",
      " 13  Romance        70904 non-null  float64\n",
      " 14  Sci-Fi         70904 non-null  float64\n",
      " 15  Slice of Life  70904 non-null  float64\n",
      " 16  Sports         70904 non-null  float64\n",
      " 17  Supernatural   70904 non-null  float64\n",
      " 18  Suspense       70904 non-null  float64\n",
      "dtypes: float64(17), object(2)\n",
      "memory usage: 10.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_filtered_tokens.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "From the looks of it, there are rows with empty target values i.e. genre value .\n",
    "\n",
    "So we need to drop those now . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous dataset size: 71548\n",
      "Cleaned dataset size (after dropping rows with all genre nulls): 70904\n"
     ]
    }
   ],
   "source": [
    "# # List of genre columns\n",
    "# genre_columns = [\n",
    "#     \"Action\", \"Adventure\", \"Boys Love\", \"Comedy\", \"Drama\", \"Ecchi\", \n",
    "#     \"Erotica\", \"Fantasy\", \"Girls Love\", \"Horror\", \"Mystery\", \n",
    "#     \"Romance\", \"Sci-Fi\", \"Slice of Life\", \"Sports\", \"Supernatural\", \"Suspense\"\n",
    "# ]\n",
    "\n",
    "# # Drop rows where all genre columns are null\n",
    "# df_cleaned = df_filtered_tokens.dropna(subset=genre_columns, how='all')\n",
    "\n",
    "# # Check the result\n",
    "# print(f\"Previous dataset size: {len(df_filtered_tokens)}\")\n",
    "# print(f\"Cleaned dataset size (after dropping rows with all genre nulls): {len(df_cleaned)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous dataset size: 71548\n",
      "Cleaned dataset size (after dropping unwanted columns and rows with all genre nulls): 70904\n"
     ]
    }
   ],
   "source": [
    "# List of genre columns\n",
    "genre_columns = [\n",
    "    \"Action\", \"Adventure\", \"Boys Love\", \"Comedy\", \"Drama\", \"Ecchi\", \n",
    "    \"Erotica\", \"Fantasy\", \"Girls Love\", \"Horror\", \"Mystery\", \n",
    "    \"Romance\", \"Sci-Fi\", \"Slice of Life\", \"Sports\", \"Supernatural\", \"Suspense\"\n",
    "]\n",
    "\n",
    "remove_columns = [\"Girls Love\", \"Ecchi\", \"Erotica\", \"Boys Love\"]\n",
    "\n",
    "df_cleaned = df_filtered_tokens.drop(columns=remove_columns)\n",
    "\n",
    "# Dropping rows where all genre columns are null\n",
    "\n",
    "filtered_genere = [col for col in genre_columns if col not in remove_columns]\n",
    "\n",
    "df_cleaned = df_cleaned.dropna(subset=filtered_genere, how='all')\n",
    "\n",
    "print(f\"Previous dataset size: {len(df_filtered_tokens)}\")\n",
    "print(f\"Cleaned dataset size (after dropping unwanted columns and rows with all genre nulls): {len(df_cleaned)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_columns = filtered_genere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Title  \\\n",
      "0                Shingeki no Kyojin   \n",
      "1  Fullmetal Alchemist: Brotherhood   \n",
      "2                     One Punch Man   \n",
      "3                  Kimetsu no Yaiba   \n",
      "4                  Sword Art Online   \n",
      "\n",
      "                                            Synopsis  Action  Adventure  \\\n",
      "0  Centuries ago, mankind was slaughtered to near...     1.0        0.0   \n",
      "1  After a horrific alchemy experiment goes wrong...     1.0        1.0   \n",
      "2  The seemingly unimpressive Saitama has a rathe...     1.0        0.0   \n",
      "3  Ever since the death of his father, the burden...     1.0        0.0   \n",
      "4  Ever since the release of the innovative Nerve...     1.0        1.0   \n",
      "\n",
      "   Comedy  Drama  Fantasy  Horror  Mystery  Romance  Sci-Fi  Slice of Life  \\\n",
      "0     0.0    1.0      0.0     0.0      0.0      0.0     0.0            0.0   \n",
      "1     0.0    1.0      1.0     0.0      0.0      0.0     0.0            0.0   \n",
      "2     1.0    0.0      0.0     0.0      0.0      0.0     0.0            0.0   \n",
      "3     0.0    0.0      0.0     0.0      0.0      0.0     0.0            0.0   \n",
      "4     0.0    0.0      1.0     0.0      0.0      1.0     0.0            0.0   \n",
      "\n",
      "   Sports  Supernatural  Suspense  \n",
      "0     0.0           0.0       1.0  \n",
      "1     0.0           0.0       0.0  \n",
      "2     0.0           0.0       0.0  \n",
      "3     0.0           1.0       0.0  \n",
      "4     0.0           0.0       0.0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 70904 entries, 0 to 91247\n",
      "Data columns (total 15 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Title          70904 non-null  object \n",
      " 1   Synopsis       70904 non-null  object \n",
      " 2   Action         70904 non-null  float64\n",
      " 3   Adventure      70904 non-null  float64\n",
      " 4   Comedy         70904 non-null  float64\n",
      " 5   Drama          70904 non-null  float64\n",
      " 6   Fantasy        70904 non-null  float64\n",
      " 7   Horror         70904 non-null  float64\n",
      " 8   Mystery        70904 non-null  float64\n",
      " 9   Romance        70904 non-null  float64\n",
      " 10  Sci-Fi         70904 non-null  float64\n",
      " 11  Slice of Life  70904 non-null  float64\n",
      " 12  Sports         70904 non-null  float64\n",
      " 13  Supernatural   70904 non-null  float64\n",
      " 14  Suspense       70904 non-null  float64\n",
      "dtypes: float64(13), object(2)\n",
      "memory usage: 8.7+ MB\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned.head())\n",
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the status of our Generes after the final touch up with our data . \n",
    "\n",
    "Based on these values, we need to decide and select various data samplaing and training \n",
    "\n",
    "techniques to confonsate for the imbalance of data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action           23830.0\n",
      "Comedy           23174.0\n",
      "Fantasy          22237.0\n",
      "Romance          17996.0\n",
      "Drama            17720.0\n",
      "Adventure        15923.0\n",
      "Supernatural     12399.0\n",
      "Sci-Fi           12166.0\n",
      "Slice of Life     7502.0\n",
      "Mystery           6600.0\n",
      "Horror            4068.0\n",
      "Suspense          1996.0\n",
      "Sports            1663.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Synopsis</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Slice of Life</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Supernatural</th>\n",
       "      <th>Suspense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shingeki no Kyojin</td>\n",
       "      <td>Centuries ago, mankind was slaughtered to near...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>After a horrific alchemy experiment goes wrong...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One Punch Man</td>\n",
       "      <td>The seemingly unimpressive Saitama has a rathe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kimetsu no Yaiba</td>\n",
       "      <td>Ever since the death of his father, the burden...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sword Art Online</td>\n",
       "      <td>Ever since the release of the innovative Nerve...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Title  \\\n",
       "0                Shingeki no Kyojin   \n",
       "1  Fullmetal Alchemist: Brotherhood   \n",
       "2                     One Punch Man   \n",
       "3                  Kimetsu no Yaiba   \n",
       "4                  Sword Art Online   \n",
       "\n",
       "                                            Synopsis  Action  Adventure  \\\n",
       "0  Centuries ago, mankind was slaughtered to near...     1.0        0.0   \n",
       "1  After a horrific alchemy experiment goes wrong...     1.0        1.0   \n",
       "2  The seemingly unimpressive Saitama has a rathe...     1.0        0.0   \n",
       "3  Ever since the death of his father, the burden...     1.0        0.0   \n",
       "4  Ever since the release of the innovative Nerve...     1.0        1.0   \n",
       "\n",
       "   Comedy  Drama  Fantasy  Horror  Mystery  Romance  Sci-Fi  Slice of Life  \\\n",
       "0     0.0    1.0      0.0     0.0      0.0      0.0     0.0            0.0   \n",
       "1     0.0    1.0      1.0     0.0      0.0      0.0     0.0            0.0   \n",
       "2     1.0    0.0      0.0     0.0      0.0      0.0     0.0            0.0   \n",
       "3     0.0    0.0      0.0     0.0      0.0      0.0     0.0            0.0   \n",
       "4     0.0    0.0      1.0     0.0      0.0      1.0     0.0            0.0   \n",
       "\n",
       "   Sports  Supernatural  Suspense  \n",
       "0     0.0           0.0       1.0  \n",
       "1     0.0           0.0       0.0  \n",
       "2     0.0           0.0       0.0  \n",
       "3     0.0           1.0       0.0  \n",
       "4     0.0           0.0       0.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_columns = df_cleaned.columns[2:] \n",
    "\n",
    "genre_counts = df_cleaned[genre_columns].sum()\n",
    "\n",
    "genre_counts = genre_counts.sort_values(ascending=False)\n",
    "\n",
    "print(genre_counts)\n",
    "\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Model**                | **Size**            | **Speed**                   | **Memory Requirements**  | **Reason to Choose** | **Why Not** |\n",
    "|--------------------------|---------------------|-----------------------------|--------------------------|----------------------|-------------|\n",
    "| **BERT (bert-base-uncased)** | Medium (110M parameters) | Slow to moderate           | High (~12GB for fine-tuning) | Popular, well-supported for classification tasks | High memory requirements for training |\n",
    "| **DistilBERT (distilbert-base-uncased)** | Smaller (66M parameters) | Faster than BERT           | Moderate (~8GB for fine-tuning) | Faster and lighter version of BERT, good for resource-constrained setups | Slightly lower accuracy than full BERT |\n",
    "| **RoBERTa (roberta-base)** | Large (125M parameters) | Slow to moderate           | High (~13GB for fine-tuning) | Improved BERT architecture, good for NLP tasks | High memory usage, slower processing |\n",
    "| **ALBERT (albert-base-v2)** | Small (12M parameters) | Fast                       | Low (~5GB for fine-tuning)  | Efficient, reduces model size, still effective for NLP | Somewhat lower accuracy compared to BERT and RoBERTa |\n",
    "| **XLNet (xlnet-base-cased)** | Large (110M parameters) | Slow to moderate           | High (~12GB for fine-tuning) | Great for NLP tasks, powerful but complex | Higher memory usage, slower inference |\n",
    "| **Longformer (allenai/longformer-base-4096)** | Large (125M parameters) | Moderate                   | High (~12GB for fine-tuning) | Best for long sequences (large token window) | Not ideal for short data (like 512 tokens max) |\n",
    "| **GPT-2 (gpt2)** | Large (124M parameters) | Moderate                   | High (~12GB for fine-tuning) | Good for generative tasks, less suited for classification | Not designed for classification, slower for non-generative tasks |\n",
    "| **T5 (t5-small)** | Small (60M parameters) | Moderate                   | Moderate (~8GB for fine-tuning) | Good for text generation and translation tasks, flexible | Not as optimized for classification tasks |\n",
    "| **BART (facebook/bart-base)** | Large (140M parameters) | Moderate                   | High (~13GB for fine-tuning) | Good for sequence-to-sequence tasks, robust for text generation | High memory usage, slower inference |\n",
    "| **ULMFiT (AWD-LSTM - fastai)** | Medium (50M parameters) | Fast                       | Moderate (~8GB for fine-tuning) | Works well with transfer learning, fast training, optimized for smaller datasets | May not perform as well as transformer-based models |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended Models for my system:\n",
    "\n",
    "1. **DistilBERT**:\n",
    "   - **Why**: Its a smaller, faster, and more memory-efficient version of BERT with nearly the same performance. Given our data (70K samples, 512 tokens), it will balance speed and performance on our system well.\n",
    "\n",
    "2. **ALBERT**:\n",
    "   - **Why**: ALBERTs architecture is more efficient, requiring less memory while maintaining good performance. It is ideal for my MacBook with 24GB of RAM, especially if we're looking for faster training.\n",
    "\n",
    "3. **T5 (t5-small)**:\n",
    "   - **Why**: Its smaller and more efficient, suitable for text generation and classification tasks. It will run well on our setup with moderate memory consumption.\n",
    "\n",
    "**Avoid/Consider for Larger Models**:\n",
    "\n",
    "- **BERT, RoBERTa, XLNet, BART**:\n",
    "  - **Why Not**: These models are more resource-heavy and would require significant GPU/CPU power and memory for fine-tuning, which may cause slower processing times on our system. If memory is an issue, these models might be a challenge for training with our data size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sampling techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probelm we are solving is multi-label classification . \n",
    "\n",
    "The data that we have, even after cleain is not perfect . \n",
    "\n",
    "So we need to take help of statistical decision making to aid \n",
    "\n",
    "the model steer into the direction we want it to take . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Which Model Techniques We Need and Why?**  \n",
    "\n",
    "| **Technique**                                      | **Needed?** | **Why?**  |\n",
    "|----------------------------------------------------|------------|----------|\n",
    "| **1. Weighted Binary Cross-Entropy Loss**         |  Yes     | We have some imbalanced genres (e.g., **Suspense (1996)** vs. **Action (23830)**). Without weighting, the model might focus too much on frequent genres, ignoring rare ones. Weighted BCE ensures minority labels contribute fairly to the loss.  |\n",
    "| **2. Focal Loss**                                  |  No      | Our dataset isn't extremely imbalanced. Focal Loss is useful when **minority labels are extremely rare (e.g., <1% of total data)**, which isn't the case here. Weighted BCE should be enough.  |\n",
    "| **3. Threshold Tuning for Multi-Label Classification** |  Yes     | Since multi-label classification isn't a simple \"highest probability wins\" problem, setting the right threshold per genre is **critical** to avoid over/under-predicting labels. This improves the F1-score. |\n",
    "\n",
    "---\n",
    "\n",
    "### **Sampling Techniques (During Training)**  \n",
    "\n",
    "| **Technique**                                      | **Needed?** | **Why?**  |\n",
    "|----------------------------------------------------|------------|----------|\n",
    "| **1. Stratified Sampling**                         |  Yes     | If we train in **random batches**, some batches may lack rare genres. Stratified sampling ensures each batch contains diverse labels, helping the model learn from underrepresented genres.  |\n",
    "| **2. MixUp / CutMix Augmentation**                |  No      | These techniques are **more suited for vision tasks** (e.g., images). In text, mixing two different synopses could confuse the model rather than help it learn meaningful genre relationships. |\n",
    "\n",
    "### **Final Conclusion**:\n",
    " Use **Weighted BCE Loss**  \n",
    " Apply **Threshold Tuning**  \n",
    " Implement **Stratified Sampling**  \n",
    "\n",
    "This setup ensures our model fairly learns all genres while handling class imbalance effectively! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline Order:\n",
    "\n",
    "1 Stratified Sampling \n",
    "\n",
    "2 Tokenization & Dataloader Creation \n",
    "\n",
    "3 Model Training with BCE Loss (After Tokenization)\n",
    "\n",
    "4 Threshold Tuning (After Model Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified Sampling (Before Training)\n",
    "\n",
    "Ensures each batch has a balanced proportion of majority & minority genres.\n",
    "\n",
    "\t\tSince this is multi-label classification, traditional stratified sampling wont work directly.\n",
    "\t\n",
    "\t\tInstead, use Multi-label Stratified Sampling from iterative-stratification package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Straified Tranin Test Split Creation\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Extract features & labels\n",
    "X = df_cleaned[\"Synopsis\"].values  # Summary\n",
    "y = df_cleaned.iloc[:, 2:].values  # All genre columns\n",
    "\n",
    "# Define Stratified K-Fold\n",
    "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_idx, val_idx in mskf.split(X, y):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to DataFrames\n",
    "df_train = pd.DataFrame({'Synopsis': X_train, 'Genres': list(y_train)})\n",
    "df_val = pd.DataFrame({'Synopsis': X_val, 'Genres': list(y_val)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((56723, 2), (14181, 2))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14181 entries, 0 to 14180\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Synopsis  14181 non-null  object\n",
      " 1   Genres    14181 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 221.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_columns = df_cleaned.columns[2:]  \n",
    "\n",
    "# Convert to DataFrames and select the correct genre columns\n",
    "df_train = pd.DataFrame({'Synopsis': X_train, **dict(zip(genre_columns, y_train.T))})\n",
    "df_val = pd.DataFrame({'Synopsis': X_val, **dict(zip(genre_columns, y_val.T))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Synopsis</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Slice of Life</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Supernatural</th>\n",
       "      <th>Suspense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Centuries ago, mankind was slaughtered to near...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ever since the death of his father, the burden...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ever since the release of the innovative Nerve...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The appearance of \"quirks,\" newly discovered s...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hunters devote themselves to accomplishing haz...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Synopsis  Action  Adventure  \\\n",
       "0  Centuries ago, mankind was slaughtered to near...     1.0        0.0   \n",
       "1  Ever since the death of his father, the burden...     1.0        0.0   \n",
       "2  Ever since the release of the innovative Nerve...     1.0        1.0   \n",
       "3  The appearance of \"quirks,\" newly discovered s...     1.0        0.0   \n",
       "4  Hunters devote themselves to accomplishing haz...     1.0        1.0   \n",
       "\n",
       "   Comedy  Drama  Fantasy  Horror  Mystery  Romance  Sci-Fi  Slice of Life  \\\n",
       "0     0.0    1.0      0.0     0.0      0.0      0.0     0.0            0.0   \n",
       "1     0.0    0.0      0.0     0.0      0.0      0.0     0.0            0.0   \n",
       "2     0.0    0.0      1.0     0.0      0.0      1.0     0.0            0.0   \n",
       "3     0.0    0.0      0.0     0.0      0.0      0.0     0.0            0.0   \n",
       "4     0.0    0.0      1.0     0.0      0.0      0.0     0.0            0.0   \n",
       "\n",
       "   Sports  Supernatural  Suspense  \n",
       "0     0.0           0.0       1.0  \n",
       "1     0.0           1.0       0.0  \n",
       "2     0.0           0.0       0.0  \n",
       "3     0.0           0.0       0.0  \n",
       "4     0.0           0.0       0.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader and Tokenization (DistilBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512]) torch.Size([8, 512]) torch.Size([8, 13])\n"
     ]
    }
   ],
   "source": [
    "# DistilBERT Tokenization and PyTorch Dataset Creation\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "genre_columns = df_cleaned.columns[2:]  \n",
    "\n",
    "# Custom PyTorch Dataset to handle tokenization and label conversion\n",
    "class AnimeDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - df: Pandas DataFrame containing \"Synopsis\" and genre labels.\n",
    "        - tokenizer: Tokenizer (DistilBERT in this case).\n",
    "        - max_length: Maximum token length (512 for BERT-like models).\n",
    "        \"\"\"\n",
    "        self.df = df.copy()  \n",
    "        self.tokenizer = tokenizer  \n",
    "        self.max_length = max_length  \n",
    "\n",
    "        # Ensure genre columns are numeric (convert and fill NaN with 0)\n",
    "        self.df[genre_columns] = self.df[genre_columns].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns a tokenized sample and its corresponding labels.\"\"\"\n",
    "        row = self.df.iloc[idx]  \n",
    "        \n",
    "        # Tokenize the Synopsis column value\n",
    "        encoding = self.tokenizer(\n",
    "            row[\"Synopsis\"],  \n",
    "            padding=\"max_length\",  # Pad all sequences to the max length\n",
    "            truncation=True,  # Truncate if input is longer than max_length\n",
    "            max_length=self.max_length,  # Set max token length\n",
    "            return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        # Convert genre labels to a tensor (multi-label classification)\n",
    "        labels = torch.tensor(row[genre_columns].values.astype(float), dtype=torch.float)\n",
    "        \"\"\"\n",
    "        Input id's are essentially the tokenization mapping id. \n",
    "        Attention mask helps distinguish the paddings. \n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),  # Tokenized input IDs\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),  # Attention mask\n",
    "            \"labels\": labels  # Multi-label target vector\n",
    "        }\n",
    "\n",
    "# Create dataset instances for training and validation\n",
    "train_dataset = AnimeDataset(df_train, tokenizer, MAX_LENGTH)\n",
    "val_dataset = AnimeDataset(df_val, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Create DataLoaders for batching and shuffling data\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Shuffle for randomness\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)  # No shuffle for validation\n",
    "\n",
    "# Fetch and inspect a single batch to verify shape\n",
    "batch = next(iter(train_loader))\n",
    "print(batch[\"input_ids\"].shape, batch[\"attention_mask\"].shape, batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training (DistilBERT) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (411494913.py, line 155)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 155\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"\"\"ArithmeticError\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# without gradient accumulation \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Use MPS if available on Mac, otherwise use CUDA if available, else CPU.\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "num_labels = 13\n",
    "\n",
    "# Load pre-trained DistilBERT model \n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", \n",
    "    num_labels=num_labels, \n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# ---------------------------\n",
    "# Compute Class Weights for Weighted BCE Loss\n",
    "# ---------------------------\n",
    "all_labels = []\n",
    "for batch in train_loader:\n",
    "    all_labels.append(batch[\"labels\"].to(device).cpu().numpy())\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "class_counts = np.sum(all_labels, axis=0)\n",
    "pos_weight = (len(all_labels) - class_counts) / (class_counts + 5e-6)\n",
    "pos_weight = torch.tensor(pos_weight, dtype=torch.float, device=device)\n",
    "\n",
    "# Define Weighted BCE Loss\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# ---------------------------\n",
    "# Optimizer & Scheduler\n",
    "# ---------------------------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "A Learning Rate Scheduler adjusts the learning rate during training. \n",
    "It is typically used to decrease the learning rate over time to help the model converge smoothly. \n",
    "It allows the model to start with a larger learning rate \n",
    "(so it can make large updates to the weights in the beginning) and \n",
    "then reduce the learning rate (so it can make smaller, more refined updates as training progresses).\n",
    "\n",
    "The function get_linear_schedule_with_warmup is a type of learning rate scheduler \n",
    "that uses linear decay with a warmup period. This means:\n",
    "\t\tThe learning rate starts from a small value (warmup).\n",
    "\t\tIt gradually increases to a predefined value (typically the original learning rate) \n",
    "        during the warmup phase.\n",
    "\t\tAfter the warmup period, the learning rate decreases linearly towards 0.\n",
    " \n",
    " \"\"\"\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "num_epochs = 5\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=int(0.1 * total_steps), \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Save and Load Checkpoint\n",
    "# ---------------------------\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Function to save model, optimizer, and scheduler state\n",
    "def save_checkpoint(epoch, model, optimizer, scheduler, loss, checkpoint_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, checkpoint_path)\n",
    "\n",
    "# Function to load model, optimizer, and scheduler state\n",
    "def load_checkpoint(checkpoint_path, model, optimizer, scheduler):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return epoch, loss\n",
    "\n",
    "# Try to load checkpoint if exists\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.pth\")\n",
    "start_epoch = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Loading checkpoint...\")\n",
    "    start_epoch, _ = load_checkpoint(checkpoint_path, model, optimizer, scheduler)\n",
    "    print(f\"Resuming training from epoch {start_epoch + 1}.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Training Loop\n",
    "# ---------------------------\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    print(f\"Starting Epoch {epoch+1}/{num_epochs}...\")\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        # Move batch data to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass: get raw logits\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=None)\n",
    "        # Labels = None because we want to update the weights based on our optimization function\n",
    "        logits = outputs.logits  # shape: [batch_size, num_labels]\n",
    "        \"\"\"\n",
    "        Logits are the raw, unnormalized output values \n",
    "        produced by the models final layer before applying an activation function \n",
    "        \"\"\"\n",
    "        # Compute weighted BCE loss\n",
    "        loss = loss_fn(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()  # Reset gradients after updating weights\n",
    "\n",
    "        # Save checkpoint every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            save_checkpoint(epoch+1, model, optimizer, scheduler, total_loss / step, checkpoint_path)\n",
    "            print(f\"Checkpoint saved at step {step}.\")\n",
    "\n",
    "        if step % 10 == 0:  # Print every 10 steps\n",
    "            print(f\"Step {step}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Average Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint at the end of each epoch\n",
    "    save_checkpoint(epoch+1, model, optimizer, scheduler, avg_loss, checkpoint_path)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Validation Loop (Optional)\n",
    "    # ---------------------------\n",
    "    model.eval()\n",
    "    \n",
    "    \"\"\"\n",
    "    model.eval(): This sets the model to evaluation mode. \n",
    "                  It is important for certain layers like dropout and batch normalization, \n",
    "                  which behave differently during training and evaluation. \n",
    "                  In evaluation mode, dropout is turned off, and \n",
    "                  batch normalization uses running statistics instead of batch statistics.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=None)\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fn(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/83/np7cbr6j13x9w73whgdqvldw0000gn/T/ipykernel_79385/3921872245.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=13, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the checkpoint\n",
    "\n",
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "# Use MPS if available on Mac, otherwise use CUDA if available, else CPU.\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "checkpoint_path = 'checkpoints/checkpoint.pth'\n",
    "num_labels = 13\n",
    "\n",
    "# Initialize the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", \n",
    "    num_labels=num_labels, \n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "# Load the model state dict\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move model to the correct device (e.g., GPU or CPU)\n",
    "model.to(device)\n",
    "\n",
    "# If you need to use the model for inference now, it's ready!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threshold Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to find an activation threshold for each label \n",
    "\n",
    "for which the F1 score is maximum . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: Best Threshold = 0.40, Best F1-score = 0.5623\n",
      "Label 1: Best Threshold = 0.50, Best F1-score = 0.4793\n",
      "Label 2: Best Threshold = 0.40, Best F1-score = 0.4998\n",
      "Label 3: Best Threshold = 0.40, Best F1-score = 0.4149\n",
      "Label 4: Best Threshold = 0.50, Best F1-score = 0.5415\n",
      "Label 5: Best Threshold = 0.70, Best F1-score = 0.2135\n",
      "Label 6: Best Threshold = 0.70, Best F1-score = 0.2980\n",
      "Label 7: Best Threshold = 0.40, Best F1-score = 0.4427\n",
      "Label 8: Best Threshold = 0.60, Best F1-score = 0.4662\n",
      "Label 9: Best Threshold = 0.60, Best F1-score = 0.2936\n",
      "Label 10: Best Threshold = 0.80, Best F1-score = 0.1797\n",
      "Label 11: Best Threshold = 0.50, Best F1-score = 0.3917\n",
      "Label 12: Best Threshold = 0.70, Best F1-score = 0.1040\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def find_best_thresholds(model, dataloader, device, threshold_range=np.arange(0.1, 1.0, 0.1)):\n",
    "    \"\"\"\n",
    "    Finds the best threshold for each label to maximize the F1-score.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained multi-label classification model.\n",
    "        dataloader: Validation/test DataLoader.\n",
    "        device: Device (cuda or cpu).\n",
    "        threshold_range: Range of threshold values to search.\n",
    "\n",
    "    Returns:\n",
    "        best_thresholds: Dictionary with optimal thresholds per class.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].cpu().numpy()  # True labels\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()  # Convert logits to probabilities\n",
    "\n",
    "            all_labels.append(labels)\n",
    "            all_probs.append(probs)\n",
    "\n",
    "    all_labels = np.vstack(all_labels)  # Shape: (num_samples, num_classes)\n",
    "    all_probs = np.vstack(all_probs)  # Shape: (num_samples, num_classes)\n",
    "\n",
    "    num_classes = all_labels.shape[1]\n",
    "    best_thresholds = {}\n",
    "\n",
    "    # Iterate over each class\n",
    "    for i in range(num_classes):\n",
    "        best_f1 = 0\n",
    "        best_thresh = 0.5  # Default threshold\n",
    "        \n",
    "        # Search for best threshold in the range\n",
    "        for threshold in threshold_range:\n",
    "            preds = (all_probs[:, i] >= threshold).astype(int)  # Apply threshold\n",
    "            f1 = f1_score(all_labels[:, i], preds, zero_division=0)\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_thresh = threshold\n",
    "        \n",
    "        best_thresholds[i] = best_thresh  # Store best threshold for this class\n",
    "        print(f\"Label {i}: Best Threshold = {best_thresh:.2f}, Best F1-score = {best_f1:.4f}\")\n",
    "\n",
    "    return best_thresholds\n",
    "\n",
    "# Run threshold tuning on validation data\n",
    "best_thresholds = find_best_thresholds(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply thresholds to the model predicitons \n",
    "\n",
    "def apply_thresholds(logits, best_thresholds):\n",
    "    \"\"\"\n",
    "    Apply optimized thresholds to model predictions.\n",
    "\n",
    "    Args:\n",
    "        logits: Raw model outputs (logits).\n",
    "        best_thresholds: Dictionary of optimal thresholds per class.\n",
    "\n",
    "    Returns:\n",
    "        Binary predictions (0 or 1) for each label.\n",
    "    \"\"\"\n",
    "    # Apply sigmoid to logits to get probabilities\n",
    "    probs = torch.sigmoid(logits).detach().to(device).cpu().numpy()\n",
    "    preds = np.zeros_like(probs)  \n",
    "    \n",
    "    # Iterate over classes to apply the best threshold for each\n",
    "    for i in range(probs.shape[1]):  \n",
    "        preds[:, i] = (probs[:, i] >= best_thresholds[i]).astype(int)\n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "logits = model(input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "best_predictions = apply_thresholds(logits, best_thresholds)\n",
    "\n",
    "# Now best_predictions contains the binary predictions (0 or 1) based on the best thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Action     0.4363    0.7908    0.5623      4766\n",
      "    Adventure     0.3704    0.6791    0.4793      3185\n",
      "       Comedy     0.3588    0.8233    0.4998      4635\n",
      "        Drama     0.2734    0.8600    0.4149      3544\n",
      "      Fantasy     0.5222    0.5622    0.5415      4447\n",
      "       Horror     0.1453    0.4029    0.2135       814\n",
      "      Mystery     0.2359    0.4045    0.2980      1320\n",
      "      Romance     0.3088    0.7816    0.4427      3599\n",
      "       Sci-Fi     0.4565    0.4764    0.4662      2433\n",
      "Slice of Life     0.2164    0.4564    0.2936      1501\n",
      "       Sports     0.1280    0.3012    0.1797       332\n",
      " Supernatural     0.2898    0.6040    0.3917      2480\n",
      "     Suspense     0.0619    0.3258    0.1040       399\n",
      "\n",
      "    micro avg     0.3294    0.6738    0.4425     33455\n",
      "    macro avg     0.2926    0.5745    0.3759     33455\n",
      " weighted avg     0.3580    0.6738    0.4546     33455\n",
      "  samples avg     0.3407    0.6677    0.4270     33455\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "best_thresholds = [0.40, 0.50, 0.40, 0.40, 0.50, 0.70, 0.70, 0.40, 0.60, 0.60, 0.80, 0.50, 0.70]\n",
    "\n",
    "# Define optimized thresholds per genre\n",
    "thresholds = {\n",
    "    \"Action\": 0.40, \"Adventure\": 0.50, \"Comedy\": 0.40, \"Drama\": 0.40, \"Fantasy\": 0.50,\n",
    "    \"Horror\": 0.70, \"Mystery\": 0.70, \"Romance\": 0.40, \"Sci-Fi\": 0.60, \"Slice of Life\": 0.60,\n",
    "    \"Sports\": 0.80, \"Supernatural\": 0.50, \"Suspense\": 0.70\n",
    "}\n",
    "\n",
    "# Convert thresholds to tensor (for batch processing)\n",
    "thresholds_tensor = torch.tensor(list(thresholds.values())).to(device)\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"Evaluates the model on the validation set using optimized thresholds.\"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=None)\n",
    "            logits = outputs.logits  # Raw predictions\n",
    "\n",
    "            # Apply sigmoid activation (converts logits to probabilities)\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            # Apply optimized thresholds\n",
    "            preds = (probs > thresholds_tensor).int()  # Convert to binary predictions\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    all_preds = np.vstack(all_preds)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    report = classification_report(all_labels, all_preds, target_names=thresholds.keys(), digits=4)\n",
    "    print(report)\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_model(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Classification Metrics**\n",
    "\n",
    "When evaluating a classification model, we use these key metrics:\n",
    "\n",
    "- **Precision**: Out of all the predictions for a genre, how many were actually correct? (Lower means more false positives.)\n",
    "- **Recall**: Out of all the actual instances of a genre, how many did the model correctly identify? (Lower means more false negatives.)\n",
    "- **F1-Score**: A balance between precision and recall. Higher is better.\n",
    "- **Support**: The number of actual examples for that genre in the dataset.\n",
    "\n",
    "**Averages:**\n",
    "- **Micro Avg**: Treats all genre predictions equally.\n",
    "- **Macro Avg**: Averages across all genres, regardless of size.\n",
    "- **Weighted Avg**: Like macro, but accounts for genre sizes.\n",
    "- **Samples Avg**: Evaluates across multiple labels per sample.\n",
    "\n",
    "Higher values usually mean better performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DistilRoberta Weight Thresholld Optimized Performance**\n",
    "\n",
    "| Genre            | Precision | Recall | F1-Score | Support |\n",
    "|-----------------|-----------|--------|----------|---------|\n",
    "| Action         | 0.4363    | 0.7908 | 0.5623   | 4766    |\n",
    "| Adventure      | 0.3704    | 0.6791 | 0.4793   | 3185    |\n",
    "| Comedy        | 0.3588    | 0.8233 | 0.4998   | 4635    |\n",
    "| Drama         | 0.2734    | 0.8600 | 0.4149   | 3544    |\n",
    "| Fantasy       | 0.5222    | 0.5622 | 0.5415   | 4447    |\n",
    "| Horror        | 0.1453    | 0.4029 | 0.2135   | 814     |\n",
    "| Mystery       | 0.2359    | 0.4045 | 0.2980   | 1320    |\n",
    "| Romance       | 0.3088    | 0.7816 | 0.4427   | 3599    |\n",
    "| Sci-Fi        | 0.4565    | 0.4764 | 0.4662   | 2433    |\n",
    "| Slice of Life | 0.2164    | 0.4564 | 0.2936   | 1501    |\n",
    "| Sports        | 0.1280    | 0.3012 | 0.1797   | 332     |\n",
    "| Supernatural  | 0.2898    | 0.6040 | 0.3917   | 2480    |\n",
    "| Suspense      | 0.0619    | 0.3258 | 0.1040   | 399     |\n",
    "| **Micro Avg**  | 0.3294    | 0.6738 | 0.4425   | 33455   |\n",
    "| **Macro Avg**  | 0.2926    | 0.5745 | 0.3759   | 33455   |\n",
    "| **Weighted Avg** | 0.3580 | 0.6738 | 0.4546   | 33455   |\n",
    "| **Samples Avg**  | 0.3407 | 0.6677 | 0.4270   | 33455   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "}, '../models/distilroberta_model_with_thresholds.pth')\n",
    "\n",
    "# Save thresholds to a separate file\n",
    "torch.save(thresholds_tensor, '../models/thresholds_tensor.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/83/np7cbr6j13x9w73whgdqvldw0000gn/T/ipykernel_79385/1654219762.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('distilroberta_model_with_thresholds.pth')['model_state_dict'])\n",
      "/var/folders/83/np7cbr6j13x9w73whgdqvldw0000gn/T/ipykernel_79385/1654219762.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  thresholds_tensor = torch.load('thresholds_tensor.pth').to(device)\n"
     ]
    }
   ],
   "source": [
    "# Load the model \n",
    "\n",
    "model.load_state_dict(torch.load('../models/distilroberta_model_with_thresholds.pth')['model_state_dict'])\n",
    "\n",
    "# Load the thresholds\n",
    "\n",
    "thresholds_tensor = torch.load('../models/thresholds_tensor.pth').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/83/np7cbr6j13x9w73whgdqvldw0000gn/T/ipykernel_79385/1342940742.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('distilroberta_model_with_thresholds.pth')['model_state_dict'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description: A young warrior embarks on a quest to defeat monsters and save the world.\n",
      "Predicted Genres: Action, Adventure, Fantasy\n",
      "--------------------------------------------------\n",
      "Description: A high school romance story set in a quiet town filled with mysterious events.\n",
      "Predicted Genres: Drama, Mystery, Romance, Slice of Life, Supernatural\n",
      "--------------------------------------------------\n",
      "Description: A group of heroes fight against a dark force threatening the universe.\n",
      "Predicted Genres: Action, Adventure, Fantasy, Sci-Fi\n",
      "--------------------------------------------------\n",
      "Description: A slice-of-life anime following the daily lives of a group of friends in school.\n",
      "Predicted Genres: Comedy, Drama, Romance, Slice of Life, Sports\n",
      "--------------------------------------------------\n",
      "Description: A science fiction adventure set in a futuristic world with AI and robots.\n",
      "Predicted Genres: Action, Adventure, Sci-Fi\n",
      "--------------------------------------------------\n",
      "Description: A lovely boy falls in a pit of doom and has to abandon his love for the sake of becoming a vampire.\n",
      "Predicted Genres: Action, Drama, Horror, Romance, Supernatural, Suspense\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "def get_predicted_genres(descriptions, model, tokenizer, thresholds_tensor, device='mps'):\n",
    "    \"\"\"\n",
    "    Given a list of anime descriptions, this function predicts the genres using a pre-trained model\n",
    "    and applies individual thresholds for each genre.\n",
    "\n",
    "    Parameters:\n",
    "    - descriptions: List of strings (anime descriptions).\n",
    "    - model: Pre-trained model for sequence classification.\n",
    "    - tokenizer: Tokenizer corresponding to the model.\n",
    "    - thresholds_tensor: Tensor of thresholds for each genre (shape should be [num_labels]).\n",
    "    - device: Device to run the model on ('cpu', 'cuda', or 'mps').\n",
    "\n",
    "    Returns:\n",
    "    - A list of predicted genres for each description.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer(descriptions, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    probs = torch.sigmoid(logits)\n",
    "\n",
    "    predictions = (probs > thresholds_tensor).int()\n",
    "\n",
    "    genre_labels = ['Action', 'Adventure', 'Comedy', 'Drama', 'Fantasy', 'Horror', 'Mystery', 'Romance', 'Sci-Fi', 'Slice of Life', 'Sports', 'Supernatural', 'Suspense']\n",
    "\n",
    "    result = []\n",
    "    for i, description in enumerate(descriptions):\n",
    "        predicted_genres = [genre_labels[j] for j in range(len(genre_labels)) if predictions[i][j] == 1]\n",
    "        result.append({\n",
    "            \"description\": description,\n",
    "            \"predicted_genres\": predicted_genres\n",
    "        })\n",
    "    \n",
    "    return result\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=13).to(device)\n",
    "model.load_state_dict(torch.load('distilroberta_model_with_thresholds.pth')['model_state_dict'])\n",
    "\n",
    "test_descriptions = [\n",
    "    \"A young warrior embarks on a quest to defeat monsters and save the world.\",\n",
    "    \"A high school romance story set in a quiet town filled with mysterious events.\",\n",
    "    \"A group of heroes fight against a dark force threatening the universe.\",\n",
    "    \"A slice-of-life anime following the daily lives of a group of friends in school.\",\n",
    "    \"A science fiction adventure set in a futuristic world with AI and robots.\",\n",
    "    \"A lovely boy falls in a pit of doom and has to abandon his love for the sake of becoming a vampire.\"\n",
    "]\n",
    "\n",
    "results = get_predicted_genres(test_descriptions, model, tokenizer, thresholds_tensor)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Description: {result['description']}\")\n",
    "    print(f\"Predicted Genres: {', '.join(result['predicted_genres'])}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been converted to ONNX and saved at distilroberta_model_with_thresholds.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "import numpy as np\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example input (this should be a single sample, matching your batch size and input format)\n",
    "# Modify this part to match the expected input shape for your model\n",
    "example_input = torch.randint(0, 1000, (1, 512)).to(device)  # Example: Batch size 1, sequence length 512\n",
    "\n",
    "# Path where you want to save the ONNX model\n",
    "onnx_model_path = '../models/distilroberta_model_with_thresholds.onnx'\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    model,  # Model to be exported\n",
    "    example_input,  # Example input to trace the model\n",
    "    onnx_model_path,  # Path to save the ONNX model\n",
    "    input_names=[\"input_ids\"],  # Name of the input tensor\n",
    "    output_names=[\"logits\"],  # Name of the output tensor\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},  # Dynamic axes for batch size and sequence length\n",
    "        \"logits\": {0: \"batch_size\"}  # Dynamic axes for output (logits)\n",
    "    },\n",
    "    opset_version=17  # ONNX opset version \n",
    ")\n",
    "\n",
    "print(f\"Model has been converted to ONNX and saved at {onnx_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/83/np7cbr6j13x9w73whgdqvldw0000gn/T/ipykernel_79385/1548981088.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  thresholds_tensor = torch.load('thresholds_tensor.pth').to(device)\n"
     ]
    }
   ],
   "source": [
    "# Load the ONNX model using ONNX Runtime or similar\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load('../models/distilroberta_model_with_thresholds.onnx')\n",
    "onnx.checker.check_model(onnx_model)  # Check model's validity\n",
    "\n",
    "# Create a session with ONNX Runtime\n",
    "onnx_session = ort.InferenceSession('distilroberta_model_with_thresholds.onnx')\n",
    "\n",
    "# Load the thresholds from a separate file\n",
    "thresholds_tensor = torch.load('thresholds_tensor.pth').to(device)\n",
    "\n",
    "# Now you can apply the thresholds after getting the predictions from the ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Action     0.3547    0.9648    0.5187      4766\n",
      "    Adventure     0.4290    0.2581    0.3223      3185\n",
      "       Comedy     0.3269    0.9998    0.4927      4635\n",
      "        Drama     0.2502    0.9994    0.4003      3544\n",
      "      Fantasy     0.5410    0.3101    0.3942      4447\n",
      "       Horror     0.1901    0.0663    0.0984       814\n",
      "      Mystery     0.3180    0.0682    0.1123      1320\n",
      "      Romance     0.2541    1.0000    0.4053      3599\n",
      "       Sci-Fi     0.6132    0.1603    0.2542      2433\n",
      "Slice of Life     0.2222    0.0080    0.0154      1501\n",
      "       Sports     0.1875    0.0090    0.0172       332\n",
      " Supernatural     0.1824    0.9649    0.3069      2480\n",
      "     Suspense     0.0459    0.0226    0.0303       399\n",
      "\n",
      "    micro avg     0.2889    0.6434    0.3988     33455\n",
      "    macro avg     0.3012    0.4486    0.2591     33455\n",
      " weighted avg     0.3501    0.6434    0.3605     33455\n",
      "  samples avg     0.2872    0.6200    0.3757     33455\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_model(onnx_session, dataloader, thresholds_tensor):\n",
    "    \"\"\"Evaluates the ONNX model on the validation set using optimized thresholds.\"\"\"\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    # Loop through the dataloader\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "      #  attention_mask = batch[\"attention_mask\"].to(device)  # Corrected to attention_mask\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Prepare input for ONNX model\n",
    "        input_feed = {\n",
    "            \"input_ids\": input_ids.cpu().numpy(),\n",
    "          #  \"attention_mask\": attention_mask.cpu().numpy()  # Corrected to attention_mask\n",
    "        }\n",
    "\n",
    "        # Forward pass with ONNX Runtime\n",
    "        onnx_output = onnx_session.run(None, input_feed)\n",
    "        logits = torch.tensor(onnx_output[0]).to(device)  # Convert ONNX output to tensor\n",
    "\n",
    "        # Apply sigmoid activation (converts logits to probabilities)\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        # Apply optimized thresholds\n",
    "        preds = (probs > thresholds_tensor).int()  # Convert to binary predictions\n",
    "\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    all_preds = np.vstack(all_preds)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    report = classification_report(all_labels, all_preds, target_names=thresholds.keys(), digits=4)\n",
    "    print(report)\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_model(onnx_session, val_loader, thresholds_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONNX Classification Report\n",
    "\n",
    "| Genre            | Precision | Recall | F1-Score | Support |\n",
    "|-----------------|-----------|--------|----------|---------|\n",
    "| Action         | 0.3547    | 0.9648 | 0.5187   | 4766    |\n",
    "| Adventure      | 0.4290    | 0.2581 | 0.3223   | 3185    |\n",
    "| Comedy        | 0.3269    | 0.9998 | 0.4927   | 4635    |\n",
    "| Drama         | 0.2502    | 0.9994 | 0.4003   | 3544    |\n",
    "| Fantasy       | 0.5410    | 0.3101 | 0.3942   | 4447    |\n",
    "| Horror        | 0.1901    | 0.0663 | 0.0984   | 814     |\n",
    "| Mystery       | 0.3180    | 0.0682 | 0.1123   | 1320    |\n",
    "| Romance       | 0.2541    | 1.0000 | 0.4053   | 3599    |\n",
    "| Sci-Fi        | 0.6132    | 0.1603 | 0.2542   | 2433    |\n",
    "| Slice of Life | 0.2222    | 0.0080 | 0.0154   | 1501    |\n",
    "| Sports        | 0.1875    | 0.0090 | 0.0172   | 332     |\n",
    "| Supernatural  | 0.1824    | 0.9649 | 0.3069   | 2480    |\n",
    "| Suspense      | 0.0459    | 0.0226 | 0.0303   | 399     |\n",
    "| **Micro Avg**  | 0.2889    | 0.6434 | 0.3988   | 33455   |\n",
    "| **Macro Avg**  | 0.3012    | 0.4486 | 0.2591   | 33455   |\n",
    "| **Weighted Avg** | 0.3501 | 0.6434 | 0.3605   | 33455   |\n",
    "| **Samples Avg**  | 0.2872 | 0.6200 | 0.3757   | 33455   |\n",
    "\n",
    "### Conclusion\n",
    "The ONNX model performs significantly worse in many genres compared to the base DistilRoBERTa model.  \n",
    "Therefore, we **won't** be using the ONNX version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Albert Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512]) torch.Size([8, 512]) torch.Size([8, 13])\n"
     ]
    }
   ],
   "source": [
    "# # ALbert Data Loader \n",
    "\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import AlbertTokenizerFast\n",
    "# import pandas as pd  # Ensure pandas is imported\n",
    "\n",
    "# # Load ALBERT tokenizer\n",
    "# tokenizer = AlbertTokenizerFast.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "# # Define max token length (ALBERT typically handles up to 512 tokens)\n",
    "# MAX_LENGTH = 512\n",
    "\n",
    "\n",
    "# # Define a custom PyTorch Dataset for ALBERT\n",
    "# class AnimeDataset(Dataset):\n",
    "#     def __init__(self, df, tokenizer, max_length):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#         - df: Pandas DataFrame containing \"Synopsis\" and genre labels.\n",
    "#         - tokenizer: ALBERT tokenizer.\n",
    "#         - max_length: Maximum token length.\n",
    "#         \"\"\"\n",
    "#         self.df = df.copy()  # Store DataFrame safely\n",
    "#         self.tokenizer = tokenizer  # Store tokenizer\n",
    "#         self.max_length = max_length  # Store max token length\n",
    "\n",
    "#         # Convert genre columns to numeric values and handle missing values\n",
    "#         self.df[genre_columns] = self.df[genre_columns].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"Returns a tokenized sample and its corresponding labels.\"\"\"\n",
    "#         row = self.df.iloc[idx]  # Get the row at index idx\n",
    "        \n",
    "#         # Tokenize the \"Synopsis\" text using ALBERT tokenizer\n",
    "#         encoding = self.tokenizer(\n",
    "#             row[\"Synopsis\"],  # Input text (anime/manga synopsis)\n",
    "#             padding=\"max_length\",  # Pad all sequences to max length\n",
    "#             truncation=True,  # Truncate if input is longer than max_length\n",
    "#             max_length=self.max_length,  # Set max token length\n",
    "#             return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "#         )\n",
    "\n",
    "#         # Convert genre labels to a tensor (multi-label classification)\n",
    "#         labels = torch.tensor(row[genre_columns].values.astype(float), dtype=torch.float)\n",
    "\n",
    "#         return {\n",
    "#             \"input_ids\": encoding[\"input_ids\"].squeeze(0),  # Tokenized input IDs\n",
    "#             \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),  # Attention mask\n",
    "#             \"labels\": labels  # Multi-label target vector\n",
    "#         }\n",
    "\n",
    "# # Create dataset instances for training and validation\n",
    "# train_dataset = AnimeDataset(df_train, tokenizer, MAX_LENGTH)\n",
    "# val_dataset = AnimeDataset(df_val, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# # Create DataLoaders for batching and shuffling data\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Shuffle for randomness\n",
    "# val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)  # No shuffle for validation\n",
    "\n",
    "# # Fetch and inspect a single batch to verify shape\n",
    "# batch = next(iter(train_loader))\n",
    "# print(batch[\"input_ids\"].shape, batch[\"attention_mask\"].shape, batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/5...\n",
      "Step 10/7091 - Loss: 0.9574\n",
      "Step 20/7091 - Loss: 1.2739\n",
      "Step 30/7091 - Loss: 1.0471\n",
      "Step 40/7091 - Loss: 1.0394\n",
      "Step 50/7091 - Loss: 1.0910\n",
      "Step 60/7091 - Loss: 0.9729\n",
      "Step 70/7091 - Loss: 1.5224\n",
      "Step 80/7091 - Loss: 1.6469\n",
      "Step 90/7091 - Loss: 1.5818\n",
      "Model saved at step 100.\n",
      "Step 100/7091 - Loss: 1.8607\n",
      "Step 110/7091 - Loss: 1.0166\n",
      "Step 120/7091 - Loss: 1.0204\n",
      "Step 130/7091 - Loss: 1.1073\n",
      "Step 140/7091 - Loss: 1.1957\n",
      "Step 150/7091 - Loss: 1.2459\n",
      "Step 160/7091 - Loss: 1.0186\n",
      "Step 170/7091 - Loss: 1.1716\n",
      "Step 180/7091 - Loss: 1.1898\n",
      "Step 190/7091 - Loss: 1.1397\n",
      "Model saved at step 200.\n",
      "Step 200/7091 - Loss: 1.4826\n",
      "Step 210/7091 - Loss: 0.9905\n",
      "Step 220/7091 - Loss: 1.1653\n",
      "Step 230/7091 - Loss: 1.0355\n",
      "Step 240/7091 - Loss: 1.4414\n",
      "Step 250/7091 - Loss: 1.0372\n",
      "Step 260/7091 - Loss: 0.8950\n",
      "Step 270/7091 - Loss: 0.9553\n",
      "Step 280/7091 - Loss: 0.8794\n",
      "Step 290/7091 - Loss: 1.2076\n",
      "Model saved at step 300.\n",
      "Step 300/7091 - Loss: 1.0465\n",
      "Step 310/7091 - Loss: 1.2169\n",
      "Step 320/7091 - Loss: 1.0221\n",
      "Step 330/7091 - Loss: 0.9499\n",
      "Step 340/7091 - Loss: 0.8950\n",
      "Step 350/7091 - Loss: 1.2447\n",
      "Step 360/7091 - Loss: 1.3030\n",
      "Step 370/7091 - Loss: 1.3439\n",
      "Step 380/7091 - Loss: 1.3542\n",
      "Step 390/7091 - Loss: 1.4030\n",
      "Model saved at step 400.\n",
      "Step 400/7091 - Loss: 0.8971\n",
      "Step 410/7091 - Loss: 1.1103\n",
      "Step 420/7091 - Loss: 1.1501\n",
      "Step 430/7091 - Loss: 1.0474\n",
      "Step 440/7091 - Loss: 0.9408\n",
      "Step 450/7091 - Loss: 1.1614\n",
      "Step 460/7091 - Loss: 1.4098\n",
      "Step 470/7091 - Loss: 0.9767\n",
      "Step 480/7091 - Loss: 1.5419\n",
      "Step 490/7091 - Loss: 1.2377\n",
      "Model saved at step 500.\n",
      "Step 500/7091 - Loss: 1.2482\n",
      "Step 510/7091 - Loss: 1.3809\n",
      "Step 520/7091 - Loss: 1.0136\n",
      "Step 530/7091 - Loss: 0.9433\n",
      "Step 540/7091 - Loss: 0.9622\n",
      "Step 550/7091 - Loss: 1.0223\n",
      "Step 560/7091 - Loss: 0.9881\n",
      "Step 570/7091 - Loss: 1.0601\n",
      "Step 580/7091 - Loss: 1.0087\n",
      "Step 590/7091 - Loss: 0.9743\n",
      "Model saved at step 600.\n",
      "Step 600/7091 - Loss: 0.9312\n",
      "Step 610/7091 - Loss: 0.9403\n",
      "Step 620/7091 - Loss: 1.0205\n",
      "Step 630/7091 - Loss: 1.2865\n",
      "Step 640/7091 - Loss: 1.1594\n",
      "Step 650/7091 - Loss: 1.2341\n",
      "Step 660/7091 - Loss: 1.1254\n",
      "Step 670/7091 - Loss: 0.9690\n",
      "Step 680/7091 - Loss: 1.1066\n",
      "Step 690/7091 - Loss: 1.0835\n",
      "Model saved at step 700.\n",
      "Step 700/7091 - Loss: 1.0645\n",
      "Step 710/7091 - Loss: 0.9218\n",
      "Step 720/7091 - Loss: 1.2030\n",
      "Step 730/7091 - Loss: 1.2309\n",
      "Step 740/7091 - Loss: 1.5172\n",
      "Step 750/7091 - Loss: 1.7529\n",
      "Step 760/7091 - Loss: 1.1481\n",
      "Step 770/7091 - Loss: 1.1289\n",
      "Step 780/7091 - Loss: 1.2777\n",
      "Step 790/7091 - Loss: 1.0871\n",
      "Model saved at step 800.\n",
      "Step 800/7091 - Loss: 0.9152\n",
      "Step 810/7091 - Loss: 0.9783\n",
      "Step 820/7091 - Loss: 0.9416\n",
      "Step 830/7091 - Loss: 0.9873\n",
      "Step 840/7091 - Loss: 1.2108\n",
      "Step 850/7091 - Loss: 0.9976\n",
      "Step 860/7091 - Loss: 1.4898\n",
      "Step 870/7091 - Loss: 0.9660\n",
      "Step 880/7091 - Loss: 0.9811\n",
      "Step 890/7091 - Loss: 0.9967\n",
      "Model saved at step 900.\n",
      "Step 900/7091 - Loss: 0.9127\n",
      "Step 910/7091 - Loss: 1.0163\n",
      "Step 920/7091 - Loss: 1.0573\n",
      "Step 930/7091 - Loss: 1.1792\n",
      "Step 940/7091 - Loss: 0.9268\n",
      "Step 950/7091 - Loss: 1.3851\n",
      "Step 960/7091 - Loss: 1.1507\n",
      "Step 970/7091 - Loss: 1.4122\n",
      "Step 980/7091 - Loss: 1.4528\n",
      "Step 990/7091 - Loss: 1.5576\n",
      "Model saved at step 1000.\n",
      "Step 1000/7091 - Loss: 1.1056\n",
      "Step 1010/7091 - Loss: 1.2546\n",
      "Step 1020/7091 - Loss: 1.0038\n",
      "Step 1030/7091 - Loss: 1.4145\n",
      "Step 1040/7091 - Loss: 1.0140\n",
      "Step 1050/7091 - Loss: 1.4410\n",
      "Step 1060/7091 - Loss: 1.3077\n",
      "Step 1070/7091 - Loss: 0.8684\n",
      "Step 1080/7091 - Loss: 1.3047\n",
      "Step 1090/7091 - Loss: 1.1841\n",
      "Model saved at step 1100.\n",
      "Step 1100/7091 - Loss: 1.0905\n",
      "Step 1110/7091 - Loss: 1.2973\n",
      "Step 1120/7091 - Loss: 1.1423\n",
      "Step 1130/7091 - Loss: 1.1241\n",
      "Step 1140/7091 - Loss: 1.0494\n",
      "Step 1150/7091 - Loss: 1.1710\n",
      "Step 1160/7091 - Loss: 1.0691\n",
      "Step 1170/7091 - Loss: 1.2902\n",
      "Step 1180/7091 - Loss: 1.0398\n",
      "Step 1190/7091 - Loss: 1.2311\n",
      "Model saved at step 1200.\n",
      "Step 1200/7091 - Loss: 1.0515\n",
      "Step 1210/7091 - Loss: 0.9434\n",
      "Step 1220/7091 - Loss: 0.9402\n",
      "Step 1230/7091 - Loss: 1.0368\n",
      "Step 1240/7091 - Loss: 1.1006\n",
      "Step 1250/7091 - Loss: 1.0226\n",
      "Step 1260/7091 - Loss: 1.4659\n",
      "Step 1270/7091 - Loss: 0.9750\n",
      "Step 1280/7091 - Loss: 0.9429\n",
      "Step 1290/7091 - Loss: 1.0164\n",
      "Model saved at step 1300.\n",
      "Step 1300/7091 - Loss: 1.2162\n",
      "Step 1310/7091 - Loss: 1.0497\n",
      "Step 1320/7091 - Loss: 0.9439\n",
      "Step 1330/7091 - Loss: 1.3577\n",
      "Step 1340/7091 - Loss: 1.5988\n",
      "Step 1350/7091 - Loss: 1.0996\n",
      "Step 1360/7091 - Loss: 0.9283\n",
      "Step 1370/7091 - Loss: 1.4082\n",
      "Step 1380/7091 - Loss: 1.1366\n",
      "Step 1390/7091 - Loss: 1.0385\n",
      "Model saved at step 1400.\n",
      "Step 1400/7091 - Loss: 1.6433\n",
      "Step 1410/7091 - Loss: 1.3653\n",
      "Step 1420/7091 - Loss: 0.9895\n",
      "Step 1430/7091 - Loss: 0.9452\n",
      "Step 1440/7091 - Loss: 1.4717\n",
      "Step 1450/7091 - Loss: 1.1125\n",
      "Step 1460/7091 - Loss: 0.9682\n",
      "Step 1470/7091 - Loss: 1.5335\n",
      "Step 1480/7091 - Loss: 1.1490\n",
      "Step 1490/7091 - Loss: 1.2568\n",
      "Model saved at step 1500.\n",
      "Step 1500/7091 - Loss: 0.9954\n",
      "Step 1510/7091 - Loss: 1.4148\n",
      "Step 1520/7091 - Loss: 1.1733\n",
      "Step 1530/7091 - Loss: 1.0350\n",
      "Step 1540/7091 - Loss: 1.1705\n",
      "Step 1550/7091 - Loss: 0.9768\n",
      "Step 1560/7091 - Loss: 1.1882\n",
      "Step 1570/7091 - Loss: 1.0442\n",
      "Step 1580/7091 - Loss: 1.3147\n",
      "Step 1590/7091 - Loss: 1.0616\n",
      "Model saved at step 1600.\n",
      "Step 1600/7091 - Loss: 0.8796\n",
      "Step 1610/7091 - Loss: 1.0388\n",
      "Step 1620/7091 - Loss: 1.0689\n",
      "Step 1630/7091 - Loss: 1.0027\n",
      "Step 1640/7091 - Loss: 1.1056\n",
      "Step 1650/7091 - Loss: 1.2852\n",
      "Step 1660/7091 - Loss: 1.2235\n",
      "Step 1670/7091 - Loss: 1.1977\n",
      "Step 1680/7091 - Loss: 1.3070\n",
      "Step 1690/7091 - Loss: 1.0199\n",
      "Model saved at step 1700.\n",
      "Step 1700/7091 - Loss: 1.5452\n",
      "Step 1710/7091 - Loss: 1.1912\n",
      "Step 1720/7091 - Loss: 1.3739\n",
      "Step 1730/7091 - Loss: 0.9693\n",
      "Step 1740/7091 - Loss: 1.6502\n",
      "Step 1750/7091 - Loss: 1.3178\n",
      "Step 1760/7091 - Loss: 1.0955\n",
      "Step 1770/7091 - Loss: 1.0373\n",
      "Step 1780/7091 - Loss: 1.0603\n",
      "Step 1790/7091 - Loss: 1.1296\n",
      "Model saved at step 1800.\n",
      "Step 1800/7091 - Loss: 1.6538\n",
      "Step 1810/7091 - Loss: 1.4966\n",
      "Step 1820/7091 - Loss: 1.3142\n",
      "Step 1830/7091 - Loss: 1.0592\n",
      "Step 1840/7091 - Loss: 1.1541\n",
      "Step 1850/7091 - Loss: 1.2434\n",
      "Step 1860/7091 - Loss: 1.1461\n",
      "Step 1870/7091 - Loss: 1.2470\n",
      "Step 1880/7091 - Loss: 1.0319\n",
      "Step 1890/7091 - Loss: 1.1760\n",
      "Model saved at step 1900.\n",
      "Step 1900/7091 - Loss: 0.9952\n",
      "Step 1910/7091 - Loss: 1.2410\n",
      "Step 1920/7091 - Loss: 1.6945\n",
      "Step 1930/7091 - Loss: 1.4068\n",
      "Step 1940/7091 - Loss: 1.1596\n",
      "Step 1950/7091 - Loss: 1.1295\n",
      "Step 1960/7091 - Loss: 1.1366\n",
      "Step 1970/7091 - Loss: 1.3303\n",
      "Step 1980/7091 - Loss: 1.1087\n",
      "Step 1990/7091 - Loss: 1.3042\n",
      "Model saved at step 2000.\n",
      "Step 2000/7091 - Loss: 0.9703\n",
      "Step 2010/7091 - Loss: 0.9258\n",
      "Step 2020/7091 - Loss: 1.1045\n",
      "Step 2030/7091 - Loss: 1.4872\n",
      "Step 2040/7091 - Loss: 1.0075\n",
      "Step 2050/7091 - Loss: 1.5921\n",
      "Step 2060/7091 - Loss: 0.9743\n",
      "Step 2070/7091 - Loss: 1.1837\n",
      "Step 2080/7091 - Loss: 1.6190\n",
      "Step 2090/7091 - Loss: 1.1158\n",
      "Model saved at step 2100.\n",
      "Step 2100/7091 - Loss: 1.0498\n",
      "Step 2110/7091 - Loss: 1.6062\n",
      "Step 2120/7091 - Loss: 1.2502\n",
      "Step 2130/7091 - Loss: 0.9980\n",
      "Step 2140/7091 - Loss: 0.9332\n",
      "Step 2150/7091 - Loss: 1.2515\n",
      "Step 2160/7091 - Loss: 1.2393\n",
      "Step 2170/7091 - Loss: 1.2615\n",
      "Step 2180/7091 - Loss: 1.1496\n",
      "Step 2190/7091 - Loss: 1.0550\n",
      "Model saved at step 2200.\n",
      "Step 2200/7091 - Loss: 0.9342\n",
      "Step 2210/7091 - Loss: 1.0087\n",
      "Step 2220/7091 - Loss: 1.1185\n",
      "Step 2230/7091 - Loss: 1.5944\n",
      "Step 2240/7091 - Loss: 1.3107\n",
      "Step 2250/7091 - Loss: 1.0787\n",
      "Step 2260/7091 - Loss: 0.8583\n",
      "Step 2270/7091 - Loss: 0.9519\n",
      "Step 2280/7091 - Loss: 1.4834\n",
      "Step 2290/7091 - Loss: 1.2931\n",
      "Model saved at step 2300.\n",
      "Step 2300/7091 - Loss: 1.4833\n",
      "Step 2310/7091 - Loss: 1.1078\n",
      "Step 2320/7091 - Loss: 1.1402\n",
      "Step 2330/7091 - Loss: 1.4541\n",
      "Step 2340/7091 - Loss: 1.3093\n",
      "Step 2350/7091 - Loss: 0.8727\n",
      "Step 2360/7091 - Loss: 1.3689\n",
      "Step 2370/7091 - Loss: 1.2501\n",
      "Step 2380/7091 - Loss: 1.0112\n",
      "Step 2390/7091 - Loss: 1.1329\n",
      "Model saved at step 2400.\n",
      "Step 2400/7091 - Loss: 1.5334\n",
      "Step 2410/7091 - Loss: 1.7776\n",
      "Step 2420/7091 - Loss: 0.9643\n",
      "Step 2430/7091 - Loss: 1.0370\n",
      "Step 2440/7091 - Loss: 0.9729\n",
      "Step 2450/7091 - Loss: 0.9544\n",
      "Step 2460/7091 - Loss: 0.9585\n",
      "Step 2470/7091 - Loss: 1.0084\n",
      "Step 2480/7091 - Loss: 1.3827\n",
      "Step 2490/7091 - Loss: 0.9271\n",
      "Model saved at step 2500.\n",
      "Step 2500/7091 - Loss: 1.1342\n",
      "Step 2510/7091 - Loss: 1.2883\n",
      "Step 2520/7091 - Loss: 1.4885\n",
      "Step 2530/7091 - Loss: 1.2087\n",
      "Step 2540/7091 - Loss: 1.4379\n",
      "Step 2550/7091 - Loss: 1.0246\n",
      "Step 2560/7091 - Loss: 1.3233\n",
      "Step 2570/7091 - Loss: 1.1104\n",
      "Step 2580/7091 - Loss: 1.0399\n",
      "Step 2590/7091 - Loss: 1.1692\n",
      "Model saved at step 2600.\n",
      "Step 2600/7091 - Loss: 1.2858\n",
      "Step 2610/7091 - Loss: 1.0840\n",
      "Step 2620/7091 - Loss: 1.4523\n",
      "Step 2630/7091 - Loss: 1.2648\n",
      "Step 2640/7091 - Loss: 1.5886\n",
      "Step 2650/7091 - Loss: 1.0095\n",
      "Step 2660/7091 - Loss: 0.9827\n",
      "Step 2670/7091 - Loss: 1.0561\n",
      "Step 2680/7091 - Loss: 1.1951\n",
      "Step 2690/7091 - Loss: 1.2671\n",
      "Model saved at step 2700.\n",
      "Step 2700/7091 - Loss: 1.1388\n",
      "Step 2710/7091 - Loss: 1.0871\n",
      "Step 2720/7091 - Loss: 1.2095\n",
      "Step 2730/7091 - Loss: 0.9570\n",
      "Step 2740/7091 - Loss: 1.0029\n",
      "Step 2750/7091 - Loss: 1.2235\n",
      "Step 2760/7091 - Loss: 1.1017\n",
      "Step 2770/7091 - Loss: 1.0519\n",
      "Step 2780/7091 - Loss: 1.0295\n",
      "Step 2790/7091 - Loss: 1.3742\n",
      "Model saved at step 2800.\n",
      "Step 2800/7091 - Loss: 1.1377\n",
      "Step 2810/7091 - Loss: 0.9090\n",
      "Step 2820/7091 - Loss: 1.0690\n",
      "Step 2830/7091 - Loss: 1.1339\n",
      "Step 2840/7091 - Loss: 1.2547\n",
      "Step 2850/7091 - Loss: 1.1630\n",
      "Step 2860/7091 - Loss: 1.0568\n",
      "Step 2870/7091 - Loss: 1.0226\n",
      "Step 2880/7091 - Loss: 1.1667\n",
      "Step 2890/7091 - Loss: 1.3126\n",
      "Model saved at step 2900.\n",
      "Step 2900/7091 - Loss: 1.4169\n",
      "Step 2910/7091 - Loss: 1.1146\n",
      "Step 2920/7091 - Loss: 1.5986\n",
      "Step 2930/7091 - Loss: 1.0532\n",
      "Step 2940/7091 - Loss: 1.1664\n",
      "Step 2950/7091 - Loss: 1.3715\n",
      "Step 2960/7091 - Loss: 1.5483\n",
      "Step 2970/7091 - Loss: 1.4311\n",
      "Step 2980/7091 - Loss: 1.0948\n",
      "Step 2990/7091 - Loss: 1.1582\n",
      "Model saved at step 3000.\n",
      "Step 3000/7091 - Loss: 1.0044\n",
      "Step 3010/7091 - Loss: 1.1725\n",
      "Step 3020/7091 - Loss: 1.0182\n",
      "Step 3030/7091 - Loss: 0.9816\n",
      "Step 3040/7091 - Loss: 1.0498\n",
      "Step 3050/7091 - Loss: 1.1208\n",
      "Step 3060/7091 - Loss: 1.1627\n",
      "Step 3070/7091 - Loss: 1.2428\n",
      "Step 3080/7091 - Loss: 1.0096\n",
      "Step 3090/7091 - Loss: 1.0341\n",
      "Model saved at step 3100.\n",
      "Step 3100/7091 - Loss: 1.2104\n",
      "Step 3110/7091 - Loss: 1.0498\n",
      "Step 3120/7091 - Loss: 0.9087\n",
      "Step 3130/7091 - Loss: 0.9173\n",
      "Step 3140/7091 - Loss: 0.9678\n",
      "Step 3150/7091 - Loss: 0.9637\n",
      "Step 3160/7091 - Loss: 1.0107\n",
      "Step 3170/7091 - Loss: 1.1164\n",
      "Step 3180/7091 - Loss: 1.1002\n",
      "Step 3190/7091 - Loss: 1.0775\n",
      "Model saved at step 3200.\n",
      "Step 3200/7091 - Loss: 1.1443\n",
      "Step 3210/7091 - Loss: 1.2047\n",
      "Step 3220/7091 - Loss: 1.2720\n",
      "Step 3230/7091 - Loss: 1.2343\n",
      "Step 3240/7091 - Loss: 1.0536\n",
      "Step 3250/7091 - Loss: 0.9937\n",
      "Step 3260/7091 - Loss: 0.9790\n",
      "Step 3270/7091 - Loss: 1.2051\n",
      "Step 3280/7091 - Loss: 1.1016\n",
      "Step 3290/7091 - Loss: 1.2670\n",
      "Model saved at step 3300.\n",
      "Step 3300/7091 - Loss: 1.0626\n",
      "Step 3310/7091 - Loss: 1.0895\n",
      "Step 3320/7091 - Loss: 1.2058\n",
      "Step 3330/7091 - Loss: 1.4400\n",
      "Step 3340/7091 - Loss: 1.0614\n",
      "Step 3350/7091 - Loss: 1.1705\n",
      "Step 3360/7091 - Loss: 1.2077\n",
      "Step 3370/7091 - Loss: 0.9023\n",
      "Step 3380/7091 - Loss: 1.2444\n",
      "Step 3390/7091 - Loss: 0.9359\n",
      "Model saved at step 3400.\n",
      "Step 3400/7091 - Loss: 1.1226\n",
      "Step 3410/7091 - Loss: 1.1962\n",
      "Step 3420/7091 - Loss: 1.2381\n",
      "Step 3430/7091 - Loss: 0.9124\n",
      "Step 3440/7091 - Loss: 1.2609\n",
      "Step 3450/7091 - Loss: 1.0797\n",
      "Step 3460/7091 - Loss: 1.0025\n",
      "Step 3470/7091 - Loss: 1.1134\n",
      "Step 3480/7091 - Loss: 0.9811\n",
      "Step 3490/7091 - Loss: 0.9958\n",
      "Model saved at step 3500.\n",
      "Step 3500/7091 - Loss: 1.4094\n",
      "Step 3510/7091 - Loss: 1.2611\n",
      "Step 3520/7091 - Loss: 1.2050\n",
      "Step 3530/7091 - Loss: 1.0570\n",
      "Step 3540/7091 - Loss: 1.0459\n",
      "Step 3550/7091 - Loss: 1.4463\n",
      "Step 3560/7091 - Loss: 1.4964\n",
      "Step 3570/7091 - Loss: 0.9299\n",
      "Step 3580/7091 - Loss: 1.1146\n",
      "Step 3590/7091 - Loss: 1.0031\n",
      "Model saved at step 3600.\n",
      "Step 3600/7091 - Loss: 1.2984\n",
      "Step 3610/7091 - Loss: 1.3627\n",
      "Step 3620/7091 - Loss: 1.0425\n",
      "Step 3630/7091 - Loss: 0.9932\n",
      "Step 3640/7091 - Loss: 1.1116\n",
      "Step 3650/7091 - Loss: 1.1731\n",
      "Step 3660/7091 - Loss: 1.1628\n",
      "Step 3670/7091 - Loss: 1.0747\n",
      "Step 3680/7091 - Loss: 1.0640\n",
      "Step 3690/7091 - Loss: 0.9679\n",
      "Model saved at step 3700.\n",
      "Step 3700/7091 - Loss: 1.0758\n",
      "Step 3710/7091 - Loss: 1.0257\n",
      "Step 3720/7091 - Loss: 1.3464\n",
      "Step 3730/7091 - Loss: 1.0699\n",
      "Step 3740/7091 - Loss: 1.1107\n",
      "Step 3750/7091 - Loss: 0.9965\n",
      "Step 3760/7091 - Loss: 1.0472\n",
      "Step 3770/7091 - Loss: 1.4164\n",
      "Step 3780/7091 - Loss: 1.0817\n",
      "Step 3790/7091 - Loss: 1.2933\n",
      "Model saved at step 3800.\n",
      "Step 3800/7091 - Loss: 1.0783\n",
      "Step 3810/7091 - Loss: 1.1979\n",
      "Step 3820/7091 - Loss: 1.1842\n",
      "Step 3830/7091 - Loss: 1.2145\n",
      "Step 3840/7091 - Loss: 0.9855\n",
      "Step 3850/7091 - Loss: 0.9679\n",
      "Step 3860/7091 - Loss: 1.4008\n",
      "Step 3870/7091 - Loss: 0.9215\n",
      "Step 3880/7091 - Loss: 1.2414\n",
      "Step 3890/7091 - Loss: 1.0294\n",
      "Model saved at step 3900.\n",
      "Step 3900/7091 - Loss: 1.4397\n",
      "Step 3910/7091 - Loss: 1.0313\n",
      "Step 3920/7091 - Loss: 1.2523\n",
      "Step 3930/7091 - Loss: 1.1054\n",
      "Step 3940/7091 - Loss: 1.0097\n",
      "Step 3950/7091 - Loss: 0.9674\n",
      "Step 3960/7091 - Loss: 1.0344\n",
      "Step 3970/7091 - Loss: 1.0146\n",
      "Step 3980/7091 - Loss: 1.1329\n",
      "Step 3990/7091 - Loss: 1.2281\n",
      "Model saved at step 4000.\n",
      "Step 4000/7091 - Loss: 0.8448\n",
      "Step 4010/7091 - Loss: 0.9942\n",
      "Step 4020/7091 - Loss: 0.9633\n",
      "Step 4030/7091 - Loss: 1.0434\n",
      "Step 4040/7091 - Loss: 1.1342\n",
      "Step 4050/7091 - Loss: 0.9208\n",
      "Step 4060/7091 - Loss: 1.2825\n",
      "Step 4070/7091 - Loss: 1.0366\n",
      "Step 4080/7091 - Loss: 1.1140\n",
      "Step 4090/7091 - Loss: 1.2283\n",
      "Model saved at step 4100.\n",
      "Step 4100/7091 - Loss: 1.2995\n",
      "Step 4110/7091 - Loss: 1.1243\n",
      "Step 4120/7091 - Loss: 0.9266\n",
      "Step 4130/7091 - Loss: 1.0151\n",
      "Step 4140/7091 - Loss: 0.8895\n",
      "Step 4150/7091 - Loss: 0.8558\n",
      "Step 4160/7091 - Loss: 0.9747\n",
      "Step 4170/7091 - Loss: 1.0629\n",
      "Step 4180/7091 - Loss: 1.0990\n",
      "Step 4190/7091 - Loss: 0.8657\n",
      "Model saved at step 4200.\n",
      "Step 4200/7091 - Loss: 1.2194\n",
      "Step 4210/7091 - Loss: 0.9517\n",
      "Step 4220/7091 - Loss: 1.0435\n",
      "Step 4230/7091 - Loss: 1.1213\n",
      "Step 4240/7091 - Loss: 1.3415\n",
      "Step 4250/7091 - Loss: 1.2469\n",
      "Step 4260/7091 - Loss: 1.0693\n",
      "Step 4270/7091 - Loss: 0.9327\n",
      "Step 4280/7091 - Loss: 0.9343\n",
      "Step 4290/7091 - Loss: 1.0121\n",
      "Model saved at step 4300.\n",
      "Step 4300/7091 - Loss: 0.8002\n",
      "Step 4310/7091 - Loss: 0.9542\n",
      "Step 4320/7091 - Loss: 1.5759\n",
      "Step 4330/7091 - Loss: 0.9091\n",
      "Step 4340/7091 - Loss: 1.0186\n",
      "Step 4350/7091 - Loss: 1.4079\n",
      "Step 4360/7091 - Loss: 1.2220\n",
      "Step 4370/7091 - Loss: 1.0485\n",
      "Step 4380/7091 - Loss: 0.8997\n",
      "Step 4390/7091 - Loss: 1.2002\n",
      "Model saved at step 4400.\n",
      "Step 4400/7091 - Loss: 1.0732\n",
      "Step 4410/7091 - Loss: 1.0537\n",
      "Step 4420/7091 - Loss: 1.0319\n",
      "Step 4430/7091 - Loss: 0.9589\n",
      "Step 4440/7091 - Loss: 0.9677\n",
      "Step 4450/7091 - Loss: 1.0947\n",
      "Step 4460/7091 - Loss: 0.9003\n",
      "Step 4470/7091 - Loss: 0.9037\n",
      "Step 4480/7091 - Loss: 1.6371\n",
      "Step 4490/7091 - Loss: 1.6028\n",
      "Model saved at step 4500.\n",
      "Step 4500/7091 - Loss: 0.9178\n",
      "Step 4510/7091 - Loss: 0.9090\n",
      "Step 4520/7091 - Loss: 1.2377\n",
      "Step 4530/7091 - Loss: 1.4143\n",
      "Step 4540/7091 - Loss: 1.3923\n",
      "Step 4550/7091 - Loss: 0.8865\n",
      "Step 4560/7091 - Loss: 1.0348\n",
      "Step 4570/7091 - Loss: 0.8365\n",
      "Step 4580/7091 - Loss: 0.8777\n",
      "Step 4590/7091 - Loss: 1.3712\n",
      "Model saved at step 4600.\n",
      "Step 4600/7091 - Loss: 1.1945\n",
      "Step 4610/7091 - Loss: 0.8807\n",
      "Step 4620/7091 - Loss: 1.1428\n",
      "Step 4630/7091 - Loss: 0.9340\n",
      "Step 4640/7091 - Loss: 0.9848\n",
      "Step 4650/7091 - Loss: 1.0585\n",
      "Step 4660/7091 - Loss: 1.1965\n",
      "Step 4670/7091 - Loss: 1.3257\n",
      "Step 4680/7091 - Loss: 0.9053\n",
      "Step 4690/7091 - Loss: 1.1455\n",
      "Model saved at step 4700.\n",
      "Step 4700/7091 - Loss: 0.8487\n",
      "Step 4710/7091 - Loss: 0.9217\n",
      "Step 4720/7091 - Loss: 1.1711\n",
      "Step 4730/7091 - Loss: 1.0279\n",
      "Step 4740/7091 - Loss: 1.2023\n",
      "Step 4750/7091 - Loss: 1.1861\n",
      "Step 4760/7091 - Loss: 0.9964\n",
      "Step 4770/7091 - Loss: 1.1148\n",
      "Step 4780/7091 - Loss: 1.0005\n",
      "Step 4790/7091 - Loss: 1.3462\n",
      "Model saved at step 4800.\n",
      "Step 4800/7091 - Loss: 1.0890\n",
      "Step 4810/7091 - Loss: 0.8758\n",
      "Step 4820/7091 - Loss: 1.2205\n",
      "Step 4830/7091 - Loss: 0.8018\n",
      "Step 4840/7091 - Loss: 1.2889\n",
      "Step 4850/7091 - Loss: 0.9825\n",
      "Step 4860/7091 - Loss: 1.1520\n",
      "Step 4870/7091 - Loss: 1.2906\n",
      "Step 4880/7091 - Loss: 0.7565\n",
      "Step 4890/7091 - Loss: 1.2720\n",
      "Model saved at step 4900.\n",
      "Step 4900/7091 - Loss: 1.0816\n",
      "Step 4910/7091 - Loss: 0.9573\n",
      "Step 4920/7091 - Loss: 1.3668\n",
      "Step 4930/7091 - Loss: 1.3079\n",
      "Step 4940/7091 - Loss: 0.9182\n",
      "Step 4950/7091 - Loss: 1.1101\n",
      "Step 4960/7091 - Loss: 0.9978\n",
      "Step 4970/7091 - Loss: 1.3001\n",
      "Step 4980/7091 - Loss: 1.4898\n",
      "Step 4990/7091 - Loss: 0.8148\n",
      "Model saved at step 5000.\n",
      "Step 5000/7091 - Loss: 1.0842\n",
      "Step 5010/7091 - Loss: 1.2420\n",
      "Step 5020/7091 - Loss: 1.1260\n",
      "Step 5030/7091 - Loss: 0.9208\n",
      "Step 5040/7091 - Loss: 1.0523\n",
      "Step 5050/7091 - Loss: 0.9412\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Forward pass: get logits\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits  \u001b[38;5;66;03m# shape: [batch_size, num_labels]\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/transformers/models/albert/modeling_albert.py:1144\u001b[0m, in \u001b[0;36mAlbertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1144\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1156\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1158\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/transformers/models/albert/modeling_albert.py:804\u001b[0m, in \u001b[0;36mAlbertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    800\u001b[0m     extended_attention_mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m extended_attention_mask) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m    802\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 804\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    813\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    815\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler_activation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output[:, \u001b[38;5;241m0\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/transformers/models/albert/modeling_albert.py:535\u001b[0m, in \u001b[0;36mAlbertTransformer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Index of the hidden group\u001b[39;00m\n\u001b[1;32m    533\u001b[0m group_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(i \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_groups))\n\u001b[0;32m--> 535\u001b[0m layer_group_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malbert_layer_groups\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroup_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroup_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlayers_per_group\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlayers_per_group\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_group_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/transformers/models/albert/modeling_albert.py:487\u001b[0m, in \u001b[0;36mAlbertLayerGroup.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    484\u001b[0m layer_attentions \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_index, albert_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malbert_layers):\n\u001b[0;32m--> 487\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[43malbert_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/transformers/models/albert/modeling_albert.py:450\u001b[0m, in \u001b[0;36mAlbertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    444\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m     output_hidden_states: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    449\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 450\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m     ffn_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_chunk,\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward,\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim,\n\u001b[1;32m    456\u001b[0m         attention_output[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    457\u001b[0m     )\n\u001b[1;32m    458\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_layer_layer_norm(ffn_output \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/transformers/models/albert/modeling_albert.py:416\u001b[0m, in \u001b[0;36mAlbertSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    413\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m attention_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    414\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m attention_output\u001b[38;5;241m.\u001b[39mreshape(batch_size, seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n\u001b[0;32m--> 416\u001b[0m projected_context_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m projected_context_layer_dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dropout(projected_context_layer)\n\u001b[1;32m    418\u001b[0m layernormed_context_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m projected_context_layer_dropout)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/otakutag/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from torch.optim import AdamW\n",
    "# from transformers import AlbertForSequenceClassification, AlbertTokenizerFast, get_linear_schedule_with_warmup\n",
    "# import numpy as np\n",
    "\n",
    "# # Set new learning rate for resumed training\n",
    "# new_lr = 5e-6\n",
    "# gradient_accumulation_steps = 4  # Accumulate gradients over 4 steps (for example)\n",
    "\n",
    "# # Define checkpoint path\n",
    "# checkpoint_path = os.path.join(\"checkpoints\", \"albert_checkpoint.pth\")\n",
    "\n",
    "# # Load ALBERT tokenizer\n",
    "# tokenizer = AlbertTokenizerFast.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "# # Load ALBERT model with transfer learning\n",
    "# model = AlbertForSequenceClassification.from_pretrained(\n",
    "#     \"albert-base-v2\", \n",
    "#     num_labels=num_labels, \n",
    "#     problem_type=\"multi_label_classification\"\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "# # ---------------------------\n",
    "# # Compute Class Weights\n",
    "# # ---------------------------\n",
    "# all_labels = []\n",
    "# for batch in train_loader:\n",
    "#     all_labels.append(batch[\"labels\"].to(device).cpu().numpy())\n",
    "# all_labels = np.concatenate(all_labels, axis=0)\n",
    "# class_counts = np.sum(all_labels, axis=0)\n",
    "# pos_weight = (len(all_labels) - class_counts) / (class_counts + 1e-5)  # To avoid division by zero\n",
    "# pos_weight = torch.tensor(pos_weight, dtype=torch.float, device=device)\n",
    "\n",
    "# # Define loss function with class weights\n",
    "# loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# # Define optimizer\n",
    "# optimizer = AdamW(model.parameters(), lr=new_lr)\n",
    "\n",
    "# # Scheduler setup (adjust as needed)\n",
    "# total_steps = len(train_loader) * 1  # One more epoch\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "#                                             num_warmup_steps=int(0.1 * total_steps), \n",
    "#                                             num_training_steps=total_steps)\n",
    "\n",
    "# # ---------------------------\n",
    "# # Load Checkpoint Function\n",
    "# # ---------------------------\n",
    "# def load_checkpoint(checkpoint_path, model, optimizer, scheduler):\n",
    "#     checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "#     epoch = checkpoint['epoch']\n",
    "#     loss = checkpoint['loss']\n",
    "#     return epoch, loss\n",
    "\n",
    "# #---------------------------\n",
    "# # Save Checkpoint Function\n",
    "# #---------------------------\n",
    "# def save_checkpoint(epoch, model, optimizer, scheduler, loss, checkpoint_path):\n",
    "#     torch.save({\n",
    "#         'epoch': epoch,\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         'scheduler_state_dict': scheduler.state_dict(),\n",
    "#         'loss': loss,\n",
    "#     }, checkpoint_path)\n",
    "\n",
    "# # ---------------------------\n",
    "# # Check if Checkpoint Exists and Load it\n",
    "# # ---------------------------\n",
    "# start_epoch = 0\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "#     start_epoch, _ = load_checkpoint(checkpoint_path, model, optimizer, scheduler)\n",
    "#     print(f\"Resuming training from epoch {start_epoch + 1}.\")\n",
    "\n",
    "# # Training loop for 5 epochs with gradient accumulation\n",
    "# for epoch in range(start_epoch, 5):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     print(f\"Starting Epoch {epoch + 1}/5...\")\n",
    "\n",
    "#     accumulated_steps = 0  # Track how many steps have been accumulated\n",
    "\n",
    "#     for step, batch in enumerate(train_loader, start=1):\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Move batch data to device\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#         # Forward pass: get logits\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=None)\n",
    "#         logits = outputs.logits  # shape: [batch_size, num_labels]\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = loss_fn(logits, labels)\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Backward pass (accumulating gradients)\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Only update weights after accumulating gradients for 'gradient_accumulation_steps' steps\n",
    "#         if (step % gradient_accumulation_steps == 0) or (step == len(train_loader)):\n",
    "#             optimizer.step()  # Update the weights after accumulation\n",
    "#             scheduler.step()  # Update the learning rate\n",
    "\n",
    "#         # Save the model checkpoint every 100 steps\n",
    "#         if step % 100 == 0:\n",
    "#             save_checkpoint(epoch + 1, model, optimizer, scheduler, total_loss / step, checkpoint_path)\n",
    "#             print(f\"Model saved at step {step}.\")\n",
    "\n",
    "#         # Print every 10 steps\n",
    "#         if step % 10 == 0:\n",
    "#             print(f\"Step {step}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     print(f\"Epoch {epoch + 1}/5 - Average Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "#     # Save the checkpoint after each epoch\n",
    "#     save_checkpoint(epoch + 1, model, optimizer, scheduler, avg_loss, checkpoint_path)\n",
    "#     print(f\"Model saved at {checkpoint_path}\")\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # Validation Loop (Optional)\n",
    "#     # ---------------------------\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in val_loader:\n",
    "#             input_ids = batch[\"input_ids\"].to(device)\n",
    "#             attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#             labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#             outputs = model(input_ids, attention_mask=attention_mask, labels=None)\n",
    "#             logits = outputs.logits\n",
    "#             loss = loss_fn(logits, labels)\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#     avg_val_loss = val_loss / len(val_loader)\n",
    "#     print(f\"Epoch {epoch + 1}/5 - Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "#     print(f\"Training complete for epoch {epoch + 1}!\")\n",
    "\n",
    "# # ---------------------------\n",
    "# # Save Checkpoint Function\n",
    "# # ---------------------------\n",
    "# def save_checkpoint(epoch, model, optimizer, scheduler, loss, path):\n",
    "#     torch.save({\n",
    "#         \"epoch\": epoch,\n",
    "#         \"model_state_dict\": model.state_dict(),\n",
    "#         \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "#         \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "#         \"loss\": loss\n",
    "#     }, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otakutag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
